{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b7fda4",
   "metadata": {},
   "source": [
    "**LAB for CLASSIFICATION MACHINE LEARNING**\n",
    "\n",
    "\n",
    "*Objective* : Predict the probability of a customer defaulting payment for the credit card the subsequent month, based on past information. The past information is provided in the dataset. This probability will help the collections team to prioritise follow up with customers who have a high propensity of defaulting. The goal of the research is to explore and exhibit the capabilities of various model agnostic interpretability techniques to generate explainable insights into prediction model outcomes\n",
    "\n",
    "*Input Data* : Cleaned and pre-processed files from Feature engineering session are used as input\n",
    "             x_train.csv, x_test.csv, y_train.csv, y_test.csv\n",
    "\n",
    "*Outcome Expected* : Model Building, Evaluation and comparison of ML algorithms across various performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694022e9",
   "metadata": {},
   "source": [
    "###                                                  Day 1 - Part_2 LAB\n",
    "\n",
    "###  Baseline Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries for data analysis\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Import Data balancing libraries\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Import models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Import tuning library from sklearn\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training & test datasets from Part1-Preprocessing part \n",
    "\n",
    "x_train=pd.read_csv('x_train.csv')\n",
    "x_test=pd.read_csv('x_test.csv')\n",
    "\n",
    "y_train=pd.read_csv('y_train.csv')\n",
    "y_test=pd.read_csv('y_test.csv')\n",
    "\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57289c3c",
   "metadata": {},
   "source": [
    "**Data Balancing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7804b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing SMOTE to handle class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "# fit predictor and target variable\n",
    "x_smote, y_smote = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "print('Original unbalanced dataset shape', len(y_train))\n",
    "print('Resampled balanced dataset shape', len(y_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e91f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving final balanced files in csv format for future reference (for Day2 LAB)\n",
    "\n",
    "pd.DataFrame(x_smote).to_csv(\"x_smote.csv\", index=None)\n",
    "pd.DataFrame(y_smote).to_csv(\"y_smote.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defe5f3f",
   "metadata": {
    "id": "1HD-O64CJaWG"
   },
   "source": [
    "### **Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d1b34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xltV-JKlH76A",
    "outputId": "7c475b05-d52e-43e5-d06b-98ae5ff16b6b"
   },
   "outputs": [],
   "source": [
    "# Import & fit the Logistic Regression Model\n",
    "start = time.time()\n",
    "\n",
    "logmodel = LogisticRegression(random_state=1)\n",
    "logmodel.fit(x_smote,y_smote)\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "# predicting the y test observations\n",
    "y_pred = logmodel.predict(x_test)\n",
    "y_train_pred = logmodel.predict(x_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f617e14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUYBmWtyNfUs",
    "outputId": "514389aa-365a-4c20-fc73-f3ab00cdae36"
   },
   "outputs": [],
   "source": [
    "#getting all scores for Logistic Regression\n",
    "log_acctr = round(accuracy_score(y_train_pred,y_smote), 3)\n",
    "log_acc = round(accuracy_score(y_pred,y_test), 3)\n",
    "\n",
    "log_prec = round(precision_score(y_pred,y_test), 3)\n",
    "log_rec = round(recall_score(y_pred,y_test), 3)\n",
    "\n",
    "log_f1 = round(f1_score(y_pred,y_test), 3)\n",
    "log_roc = round(roc_auc_score(y_pred,y_test), 3)\n",
    "log_time=stop-start\n",
    "\n",
    "results = pd.DataFrame([['Logistic Regression', log_acctr, log_acc, log_prec, log_rec, log_f1, log_roc, log_time]],\n",
    "        columns = ['Model', 'Train Accuracy', 'Test Accuracy', 'Precision', 'Recall', 'F1 Score','ROC/AUC', 'Training Time(s)'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeaec8e",
   "metadata": {
    "id": "1HD-O64CJaWG"
   },
   "source": [
    "### Task 1 - **Build a different model, Evaluate the predictions & Compare the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0891b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Model from sklearn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Model & Predict  - x_smote, y_smote, x_test, x_test\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c71dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation the model predictions using various performance metrics\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bf0d9",
   "metadata": {
    "id": "1HD-O64CJaWG"
   },
   "source": [
    "### Task 2 - **Re-train the model using data from different Balancing technique & Compare the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641dd1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import & fit the balancing technique \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff2498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new x_train and y_train to re-train the model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation the new model predictions using performance metrics and compare the results\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e409404",
   "metadata": {},
   "source": [
    "###                                                  Day 2 - Part_1\n",
    "\n",
    "###  Ensemble Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb3af9e",
   "metadata": {},
   "source": [
    "*NOTE:* Be sure to re-run the above code from the Day 1 lab ( the run should take a few seconds) in order to reuse the same input files and have the results available for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309c694",
   "metadata": {},
   "source": [
    "### **Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting data into Random Forest Classifier\n",
    "start = time.time()\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_smote,y_smote)\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "# predicting the y test observations\n",
    "y_pred = rfc.predict(x_test)\n",
    "y_train_pred = rfc.predict(x_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all scores for Random Forest Classifier\n",
    "rfc_acctr = round(accuracy_score(y_train_pred,y_smote), 3)\n",
    "rfc_acc = round(accuracy_score(y_pred,y_test), 3)\n",
    "rfc_prec = round(precision_score(y_pred,y_test), 3)\n",
    "rfc_rec = round(recall_score(y_pred,y_test), 3)\n",
    "rfc_f1 = round(f1_score(y_pred,y_test), 3)\n",
    "rfc_roc = round(roc_auc_score(y_pred,y_test), 3)\n",
    "rfc_time=stop-start\n",
    "\n",
    "results = pd.DataFrame([['Random Forest classifier', rfc_acctr, rfc_acc, rfc_prec, rfc_rec, rfc_f1, rfc_roc, rfc_time]],\n",
    "            columns = ['Model', 'Train Accuracy', 'Test Accuracy', 'Precision', 'Recall', 'F1 Score','ROC', 'Training Time(s)'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61923ff8",
   "metadata": {},
   "source": [
    "### Task 3 -  **Build any Boosting model, Evaluate the predictions & Compare the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c84330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Model from sklearn\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dead392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Model & Predict  - x_smote, y_smote, x_test, x_test\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d49fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of model predictions\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1f1c8b",
   "metadata": {},
   "source": [
    "###                                                  Day 2 - Part_2 LAB\n",
    "\n",
    "###  Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfae7aa",
   "metadata": {},
   "source": [
    "### Random Forest Classifer with tuning\n",
    "Possible parameters to tune:\n",
    "\n",
    "1. n_estimators in [10, 100, 1000]\n",
    "2. max_features in [‘sqrt’, ‘log2’]   or max_features [1 to 20]\n",
    "3. min_samples_split\n",
    "4. min_samples_leaf\n",
    "5. max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ea74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Grid\n",
    "param_dict = {'n_estimators' : [10, 50, 100],\n",
    "               'max_depth' : [2, 3, 5, 10]}\n",
    "\n",
    "# Create an instance of the RandomForestClassifier\n",
    "start = time.time()\n",
    "\n",
    "rfch = RandomForestClassifier()\n",
    "\n",
    "# Grid search\n",
    "rfch_grid = GridSearchCV(estimator=rfch,\n",
    "                       param_grid = param_dict,\n",
    "                       cv = 3, verbose=2, scoring='roc_auc')\n",
    "rfch_grid.fit(x_smote, y_smote)\n",
    "\n",
    "stop = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce1924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying best parameters\n",
    "print(rfch_grid.best_estimator_)\n",
    "print(rfch_grid.best_params_)\n",
    "\n",
    "rfch_optimal_model = rfch_grid.best_estimator_\n",
    "\n",
    "#class prediction of y on train and test\n",
    "y_pred_rfch_grid = rfch_optimal_model.predict(x_test)\n",
    "y_train_pred_rfch_grid = rfch_optimal_model.predict(x_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all scores for Random Forest Classifier\n",
    "rfch_acctr = round(accuracy_score(y_train_pred_rfch_grid,y_smote), 3)\n",
    "rfch_acc = round(accuracy_score(y_pred_rfch_grid,y_test), 3)\n",
    "rfch_prec = round(precision_score(y_pred_rfch_grid,y_test), 3)\n",
    "rfch_rec = round(recall_score(y_pred_rfch_grid,y_test), 3)\n",
    "rfch_f1 = round(f1_score(y_pred_rfch_grid,y_test), 3)\n",
    "rfch_roc = round(roc_auc_score(y_pred_rfch_grid,y_test), 3)\n",
    "rfch_time=stop-start\n",
    "\n",
    "results = pd.DataFrame([['Random Forest tuned', rfch_acctr, rfch_acc, rfch_prec, rfch_rec, rfch_f1, rfch_roc, rfch_time]],\n",
    "            columns = ['Model', 'Train Accuracy', 'Test Accuracy', 'Precision', 'Recall', 'F1 Score','ROC', 'Training Time(s)'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1079fdf",
   "metadata": {},
   "source": [
    "### Task 4  **Tune a Boosting model, Evaluate the predictions and Compare the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6442743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters and apply Gridsearch to find best parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1733f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit & Predict the tuned Model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of tuned model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd4c151",
   "metadata": {},
   "source": [
    "#### Compare the results of all 4 Tasks to see how the performance changes / improves based the following parameters:\n",
    "    1. Choice/Usage of Class Balancing techniques\n",
    "    2. Model selection\n",
    "    3. Hyperparameter tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
