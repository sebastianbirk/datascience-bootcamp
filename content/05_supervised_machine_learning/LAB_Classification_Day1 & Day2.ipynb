{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b7fda4",
   "metadata": {},
   "source": [
    "#### **LAB for CLASSIFICATION MACHINE LEARNING**\n",
    "\n",
    "\n",
    "<font color='Blue'>*Objective*:</font> Predict the probability of a customer defaulting payment for the credit card the subsequent month, based on past information. The past information is provided in the dataset. This probability will help the collections team to prioritise follow up with customers who have a high propensity of defaulting. The goal of the research is to explore and exhibit the capabilities of various model agnostic interpretability techniques to generate explainable insights into prediction model outcomes\n",
    "\n",
    "<font color='Blue'>*Input Data*:</font> Cleaned and pre-processed files from Feature engineering session are used as input.\n",
    "  - x_train.csv, x_test.csv, y_train.csv, y_test.csv <br>\n",
    "  - Location --> content/04_data_preprocessing_&_feature_engineering/Solution_Classification_preprocessing.ipynb\n",
    "             \n",
    "\n",
    "<font color='Blue'>*Outcome Expected*:</font> Build Machine learning Models, Evaluation their performace and compare ML algorithms across various performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694022e9",
   "metadata": {},
   "source": [
    "###                                                  Day 1 - Part_2 LAB\n",
    "\n",
    "####  Basic Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Import Data balancing libraries\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Import models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Import tuning library from sklearn\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training & test datasets from Part1- Feature Engineering solution \n",
    "\n",
    "x_train=pd.read_csv('../datasets/classification/processed/X_train.csv', index_col=0)\n",
    "x_test=pd.read_csv('../datasets/classification/processed/X_test.csv', index_col=0)\n",
    "\n",
    "y_train=pd.read_csv('../datasets/classification/processed/y_train.csv', index_col=0)\n",
    "y_test=pd.read_csv('../datasets/classification/processed/y_test.csv', index_col=0)\n",
    "\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57289c3c",
   "metadata": {},
   "source": [
    "#### **Data Balancing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1627fa-145a-4eb0-b32d-efeb2df406be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset requires Resampling\n",
    "print(\"Original unbalanced dataset distribution: \", y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7804b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing SMOTETomek to handle class imbalance\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "balanced_data = SMOTETomek()\n",
    "\n",
    "# fit predictor and target variable\n",
    "x_smote, y_smote = balanced_data.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c882d0d-50e7-433c-9ad8-3c8cbc6e6373",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resampled balanced dataset distribution: \", y_smote.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defe5f3f",
   "metadata": {
    "id": "1HD-O64CJaWG"
   },
   "source": [
    "#### **Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d1b34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xltV-JKlH76A",
    "outputId": "7c475b05-d52e-43e5-d06b-98ae5ff16b6b"
   },
   "outputs": [],
   "source": [
    "# Fit the Logistic Regression Model\n",
    "start = time.time()\n",
    "\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(x_smote,y_smote)\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "# predicting the y test observations\n",
    "y_pred = logmodel.predict(x_test)\n",
    "y_train_pred = logmodel.predict(x_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f617e14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUYBmWtyNfUs",
    "outputId": "514389aa-365a-4c20-fc73-f3ab00cdae36"
   },
   "outputs": [],
   "source": [
    "#Deriving all scores for Logistic Regression\n",
    "log_acctr = round(accuracy_score(y_train_pred,y_smote), 3)\n",
    "log_acc = round(accuracy_score(y_pred,y_test), 3)\n",
    "\n",
    "log_prec = round(precision_score(y_pred,y_test), 3)\n",
    "log_rec = round(recall_score(y_pred,y_test), 3)\n",
    "\n",
    "log_f1 = round(f1_score(y_pred,y_test), 3)\n",
    "log_roc = round(roc_auc_score(y_pred,y_test), 3)\n",
    "log_time=stop-start\n",
    "\n",
    "results = pd.DataFrame([['Logistic Regression', log_acctr, log_acc, log_prec, log_rec, log_f1, log_roc, log_time]],\n",
    "        columns = ['Model', 'Train Accuracy', 'Test Accuracy', 'Precision', 'Recall', 'F1 Score','ROC/AUC', 'Training Time(s)'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeaec8e",
   "metadata": {
    "id": "1HD-O64CJaWG"
   },
   "source": [
    "### Task 1 - **Build a different model, Evaluate the predictions & Compare the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0891b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Model from sklearn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Model & Predict the outcome using x_smote, y_smote, x_test, y_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c71dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model predictions using various performance metrics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bf0d9",
   "metadata": {
    "id": "1HD-O64CJaWG"
   },
   "source": [
    "### Task 2 - **Re-train the model using data from different Balancing technique & Compare the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641dd1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import & fit a different balancing technique \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff2498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new x_train and y_train to re-train the model (fit and predict) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the new model predictions using performance metrics and compare the results\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43327539-72af-4e0e-bff1-6aedd2b5224f",
   "metadata": {},
   "source": [
    "------------------------------------ **END of DAY 1 LAB** ------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f71c7-206c-42b9-8e24-9740feddb369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e409404",
   "metadata": {},
   "source": [
    "###                                                  Day 2 - Part_1\n",
    "\n",
    "####  Ensemble Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb3af9e",
   "metadata": {},
   "source": [
    "*NOTE:* Be sure to re-run the above code from the Day 1 lab ( the run should take a few seconds) in order to reuse the same input files and have the results available for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309c694",
   "metadata": {},
   "source": [
    "#### **Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting data into Random Forest Classifier\n",
    "start = time.time()\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_smote,y_smote)\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "# predicting the test observations\n",
    "y_pred = rfc.predict(x_test)\n",
    "y_train_pred = rfc.predict(x_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all scores for Random Forest Classifier\n",
    "rfc_acctr = round(accuracy_score(y_train_pred,y_smote), 3)\n",
    "rfc_acc = round(accuracy_score(y_pred,y_test), 3)\n",
    "rfc_prec = round(precision_score(y_pred,y_test), 3)\n",
    "rfc_rec = round(recall_score(y_pred,y_test), 3)\n",
    "rfc_f1 = round(f1_score(y_pred,y_test), 3)\n",
    "rfc_roc = round(roc_auc_score(y_pred,y_test), 3)\n",
    "rfc_time=stop-start\n",
    "\n",
    "results = pd.DataFrame([['Random Forest classifier', rfc_acctr, rfc_acc, rfc_prec, rfc_rec, rfc_f1, rfc_roc, rfc_time]],\n",
    "            columns = ['Model', 'Train Accuracy', 'Test Accuracy', 'Precision', 'Recall', 'F1 Score','ROC', 'Training Time(s)'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61923ff8",
   "metadata": {},
   "source": [
    "### Task 3 -  **Build a different Ensemble model, Evaluate the predictions & Compare the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c84330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a different Model from sklearn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dead392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Model & Predict the outcome - x_smote, y_smote, x_test, y_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d49fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of model predictions using various performance metrics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1f1c8b",
   "metadata": {},
   "source": [
    "###                                                  Day 2 - Part_2 LAB\n",
    "\n",
    "###  Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfae7aa",
   "metadata": {},
   "source": [
    "#### Random Forest Classifer with tuning\n",
    "Possible parameters to tune:\n",
    "\n",
    "1. n_estimators in [10, 100, 1000]\n",
    "2. max_features in [‘sqrt’, ‘log2’]   or max_features [1 to 20]\n",
    "3. min_samples_split\n",
    "4. min_samples_leaf\n",
    "5. max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ea74d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Grid\n",
    "param_dict = {'n_estimators' : [10, 50, 100],\n",
    "               'max_depth' : [2, 3, 5, 10]}\n",
    "\n",
    "# Create an instance of the RandomForestClassifier\n",
    "start = time.time()\n",
    "\n",
    "rfch = RandomForestClassifier()\n",
    "\n",
    "# Grid search\n",
    "rfch_grid = GridSearchCV(estimator=rfch,\n",
    "                       param_grid = param_dict,\n",
    "                       cv = 3, verbose=2, scoring='roc_auc')\n",
    "rfch_grid.fit(x_smote, y_smote)\n",
    "\n",
    "stop = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce1924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the best parameters\n",
    "print(rfch_grid.best_estimator_)\n",
    "print(rfch_grid.best_params_)\n",
    "\n",
    "rfch_optimal_model = rfch_grid.best_estimator_\n",
    "\n",
    "#class prediction of y on train and test sets\n",
    "y_pred_rfch_grid = rfch_optimal_model.predict(x_test)\n",
    "y_train_pred_rfch_grid = rfch_optimal_model.predict(x_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all scores for Random Forest Classifier\n",
    "rfch_acctr = round(accuracy_score(y_train_pred_rfch_grid,y_smote), 3)\n",
    "rfch_acc = round(accuracy_score(y_pred_rfch_grid,y_test), 3)\n",
    "rfch_prec = round(precision_score(y_pred_rfch_grid,y_test), 3)\n",
    "rfch_rec = round(recall_score(y_pred_rfch_grid,y_test), 3)\n",
    "rfch_f1 = round(f1_score(y_pred_rfch_grid,y_test), 3)\n",
    "rfch_roc = round(roc_auc_score(y_pred_rfch_grid,y_test), 3)\n",
    "rfch_time=stop-start\n",
    "\n",
    "results = pd.DataFrame([['Random Forest tuned', rfch_acctr, rfch_acc, rfch_prec, rfch_rec, rfch_f1, rfch_roc, rfch_time]],\n",
    "            columns = ['Model', 'Train Accuracy', 'Test Accuracy', 'Precision', 'Recall', 'F1 Score','ROC', 'Training Time(s)'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1079fdf",
   "metadata": {},
   "source": [
    "### Task 4:  **Tune the model from Task3, Evaluate the predictions and Compare the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6442743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters and apply Gridsearch to find best parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1733f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit & Predict the tuned Model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of tuned model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd4c151",
   "metadata": {},
   "source": [
    "#### Compare the results of all 4 Tasks to see how the performance changes / improves based the following parameters:\n",
    "    1. Choice/Usage of Class Balancing techniques\n",
    "    2. Model selection\n",
    "    3. Hyperparameter tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
