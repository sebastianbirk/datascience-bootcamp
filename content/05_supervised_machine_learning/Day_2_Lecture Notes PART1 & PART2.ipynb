{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d6d8c6-e542-4ebe-954c-024c85d116f0",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning Models - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c4bf7-2dca-471a-a867-ac36da82fa77",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [PART 1](#chapter1)  \n",
    "\n",
    "    * [Ensemble Learning algorithms](#section_1_1)\n",
    "        1. [Bagging algorithms](#Section_1_1_1)\n",
    "            *  [Random Forest](#section_2_1_1)\n",
    "        3. [Boosting algorithms](#section_3_1_1)\n",
    "             * [AdaBoost](#section_3_2_1)\n",
    "             * [Gradient Boosting XGBoost ](#section_3_2_2)\n",
    "             * [XGBoost ](#section_3_2_3)\n",
    "        4. [Stacking](#section_4_1_1)\n",
    "<br>\n",
    "\n",
    "* [PART 2](#chapter2)\n",
    "\n",
    "    * [Cross Validation](#section_4_1)\n",
    "    * [HyperParameter Tuning](#section_5_1)\n",
    "        * [GridsearchCV](#section_5_1_1)\n",
    "        * [RandomSearchCV](#section_5_1_2)\n",
    "    * [Model / Algorithm Selection](#section_6_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8af364-9270-4ba7-8215-825c0717ef19",
   "metadata": {},
   "source": [
    "## PART 1 <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fcb1dc-50f9-4f24-b1bc-d842f57d0c8b",
   "metadata": {},
   "source": [
    "## Ensemble Learning algorithms <a class=\"anchor\" id=\"section_1_1\"></a>\n",
    "\n",
    "Ensemble learning models are simply combinations of different machine learning models.Instead of training one large/complex model for your dataset, you train multiple small/simpler models (weak-learners) and aggregate their output (in various ways) to form your prediction.\n",
    "\n",
    "It combines multiple weak models/learners into one predictive model to **reduce bias, variance and/or improve accuracy.**\n",
    "\n",
    "<font color='Brown'>**Bias**</font>\n",
    "<font color='Brown'>**Variance**</font>\n",
    "\n",
    "<img src=\"Images/bvsv.png\" width=\"300\"/>\n",
    "\n",
    "\n",
    "<font color='Brown'>**Types of Ensemble Learning: N number of weak learners:**</font>\n",
    "1. Bagging\n",
    "2. Boosting\n",
    "3. Stacking\n",
    "\n",
    "These methods have the same wisdom-of-the-crowd concept but differ in the details of what it focuses on, the **type of weak learners** used, and the **type of aggregation** used to form the final output.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b91607-0f95-4aa7-88db-ab953213ebfd",
   "metadata": {},
   "source": [
    "### 1. Bagging algorithms <a class=\"anchor\" id=\"Section_1_1_1\"></a>\n",
    "\n",
    "**Bagging (Boostrap Aggregating):** Trains N different weak models (usually of same types – homogenous) with N non-overlapping subset of the input dataset **in parallel.** In the test phase, each model is evaluated. The label with the greatest number of predictions is selected as the prediction. Bagging methods reduces variance of the prediction.\n",
    "\n",
    "For each weak-learner, the input data is randomly sampled from the original dataset with replacement and is trained. By sampling with replacement some observations may be repeated in each new training data set. A random sampling of the subset with replacement creates nearly iid samples. During inference, the test input is fed to all the weak-learners and the output is collected. The result outputted from bagging is the average (if the problem is regression) or the most suitable label by the voting scheme (if the problem is classification).\n",
    "\n",
    "In bagging methods, the weak-learners, usually are of the same type. Since the random sampling with replacement, the bagging method doesn't change the bias in the prediction but **reduces its variance.**\n",
    "\n",
    "<img src=\"Images/bag.png\" width=\"300\"/>\n",
    "\n",
    "Each decision tree in the bag is trained on an independent subset of the training data. These subsets are random bootstraps of the whole training set. In other words, suppose the training data is a table with n observations on m features. Each component tree of the bagging will receive a subset of k observations on m features to train on, with k < n. Each observation of a subset is drawn from the full data with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a099f76e-6e62-46f1-8a2e-a011a8813b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bag = BaggingRegressor(base_estimator=DecisionTreeRegressor()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf87c9a8-87ec-4fce-be22-3bb85c871cc4",
   "metadata": {},
   "source": [
    "#### Random Forest <a class=\"anchor\" id=\"section_2_1_1\"> </a>\n",
    "\n",
    "- Random forest is very similar to bagging: it also consists of many decision trees, each of the trees is assigned with a bootstrap sample of the training data, and the final result of the meta-model is computed as the average or mode of the outputs from the components.\n",
    "\n",
    "- The only difference is that random forests, when splitting a node of a component tree, not all of the features are taken as candidates to split. Instead, only a subset of the whole feature set is selected to be the candidates (the selection is random for each node) and then the best feature from this subset is appointed to be the splitting test at that node. Suppose there are m features overall, the size of the subset can be any number from 1 to m-1.\n",
    "\n",
    "- The random forests algorithm tries to decorrelate the trees so that they learn different things about the data. It does this by selecting **a random subset of variables.** If one or a few independent variables are very strong predictors for the response variable, these features will be selected in many of the trees, causing them to become correlated. Random subsampling of independent variables ensures that not always the best predictors overall are selected for every tree and, the model does have a chance to learn other features of the data.\n",
    "\n",
    "- They are simply ensembles of decision trees. Each tree is trained with a different randomly selected part of the data with randomly selected independent variables. The goal of introducing randomness is **to reduce the variance** of the model so it does **not overfit**, at the expense of a **small increase in the bias** and **some loss of interpretability**. This strategy generally boosts the performance of the final model.\n",
    "\n",
    "<font color='Brown'>**How it works:**</font>\n",
    "\n",
    "- Individual trees are built independently, using the same procedure as for a normal decision tree but with only a random portion of the data and only considering a random subset of the features at each node. Aside from this, the training procedure is exactly the same as for an individual Decision Tree, repeated N times.\n",
    "\n",
    "- To make a prediction using a Random Forest each an individual prediction is obtained from each tree. Then, if it is a classification problem, we take the most frequent prediction as the result, and if it is a regression problem we take the average prediction from all the individual trees as the output value. The following figure illustrates how this is done:\n",
    "\n",
    "\n",
    "<img src=\"Images/randomforest.png\" width=\"700\"/>\n",
    "\n",
    "**Parameters:**\n",
    "For random forests, we have two critical arguments. \n",
    "\n",
    "- Number of Features: how many columns to use when sampling \n",
    "- Number of Trees: how many trees to average\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a42d15c-3ea8-4686-86b3-1624c26fbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr= RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382523b7-955f-4526-b640-4307bf4149be",
   "metadata": {},
   "source": [
    "### 2. Boosting algorithms <a class=\"anchor\" id=\"section_3_1_1\"></a>\n",
    "\n",
    "The general idea of boosting also encompasses building multiple weak learners to contribute to the final result. However, these component trees are built **sequentially**, one after another, and how to build the latter one is dependent on the result of the formers. Put another way, the next weak learner is built in a way to specifically improve on what the existing weak learners are bad at.\n",
    "\n",
    "There are a few answers to how should the next tree address the shortcomings of the previous trees, these answers divide boosting into several styles. The most popular styles are **AdaBoost**, **Gradient Boosting** and **XGBoost**\n",
    "\n",
    "In the test phase, each model is evaluated and based on the test error of each weak model, the prediction is weighted for voting. Boosting methods **decreases the bias of the prediction.**\n",
    "\n",
    "<img src=\"Images/boost.png\" width=\"300\"/>\n",
    "\n",
    "<font color='Brown'>**How it works:**</font>\n",
    "\n",
    "Step 1:  The base learner takes all the distributions and assign equal weight or attention to each observation.\n",
    "\n",
    "Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n",
    "\n",
    "Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n",
    "\n",
    "Finally, it combines the outputs from weak learner and creates  a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b72445-7c28-472a-8aee-e74284b55365",
   "metadata": {},
   "source": [
    "#### AdaBoost (Adaptive Boosting) <a class=\"anchor\" id=\"section_3_2_1\"></a>\n",
    "\n",
    "Adaptive Boosting aka AdaBoost fits a sequence of weak learners on different weighted training data. It starts by predicting original data set and gives equal weight to each observation. If prediction is incorrect using the first learner, then it gives higher weight to observation which have been predicted incorrectly. Being an iterative process, it continues to add learner(s) until a limit is reached in the number of models or accuracy.\n",
    "\n",
    "<font color='Brown'>**How it works:**</font>\n",
    "<img src=\"Images/ada.png\" width=\"500\"/>\n",
    "\n",
    "***Box 1:*** You can see that we have assigned equal weights to each data point and applied a decision stump to classify them as + (plus) or – (minus). The decision stump (D1) has generated vertical line at left side to classify the data points. We see that, this vertical line has incorrectly predicted three + (plus) as – (minus). In such case, we’ll assign higher weights to these three + (plus) and apply another decision stump.\n",
    "\n",
    "***Box 2:*** Here, you can see that the size of three incorrectly predicted + (plus) is bigger as compared to rest of the data points. In this case, the second decision stump (D2) will try to predict them correctly. Now, a vertical line (D2) at right side of this box has classified three mis-classified + (plus) correctly. But again, it has caused mis-classification errors. This time with three -(minus). Again, we will assign higher weight to three – (minus) and apply another decision stump.\n",
    "\n",
    "***Box 3:*** Here, three – (minus) are given higher weights. A decision stump (D3) is applied to predict these mis-classified observation correctly. This time a horizontal line is generated to classify + (plus) and – (minus) based on higher weight of mis-classified observation.\n",
    "\n",
    "***Box 4:*** Here, we have combined D1, D2 and D3 to form a strong prediction having complex rule as compared to individual weak learner. You can see that this algorithm has classified these observation quite well as compared to any of individual weak learner.\n",
    "\n",
    "We can use AdaBoost algorithms for both classification and regression problem.\n",
    "\n",
    "***The drawback of AdaBoost*** is that it is easily defeated by noisy data, the efficiency of the algorithm is highly affected by outliers as the algorithm tries to fit every point perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259dedf-e47e-46cf-9f28-818ecf5db4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier #For Classification\n",
    "from sklearn.ensemble import AdaBoostRegressor #For Regression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators = 50, base_estimator = DecisionTreeClassifier)\n",
    "clf.fit(x_train,y_train)\n",
    "clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc42f30-26bc-4d1a-a013-2c6335365b49",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "- N estimators: It controls the number of weak learners.\n",
    "- Learning Rate: Controls the contribution of weak learners in the final combination. There is a trade-off between learning_rate and n_estimators.\n",
    "- Base estimators: It helps to specify different ML algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4467e4f7-5503-4d26-910d-be4405e18ec0",
   "metadata": {},
   "source": [
    "#### Gradient Boosting <a class=\"anchor\" id=\"section_3_2_2\"></a>\n",
    "\n",
    "Gradient boosting is also based on the sequential and symbol learning model. The base learners are generated sequentially in such a way that the present based learner is always more effective than the previous one. The overall model improves sequentially with each iteration now.\n",
    "\n",
    "However, in this boosting the weights for misclassified outcomes are not incremented. The main idea here is to overcome the residual errors (prediction error for the current ensemble of models) of the previous model. It tries to optimize the loss function of previous learner by adding a new adaptive model that combines weak learners. Specifically in gradient boosting, the simple models are trees. As in random forests, many trees are grown but in this case, trees are sequentially grown and each tree focuses on fixing the shortcomings of the previous trees. \n",
    "\n",
    "<img src=\"Images/gb.png\" width=\"500\"/>\n",
    "\n",
    "Gradient boosting is a greedy algorithm and can overfit a training dataset quickly.\n",
    "\n",
    "The loss function is the one that needs to be optimized (Reduce the error) You have to keep adding a model that will regularize the loss function from the previous learner.\n",
    "Just like adaptive boosting gradient boosting can also be used for both classification and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d276241-d681-450d-8199-5b0826595011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier #For Classification\n",
    "from sklearn.ensemble import GradientBoostingRegressor #For Regression\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, loss_function = deviance, max_depth=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0621683f-503f-4e9d-8af1-083cd8ff5f2a",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "- N estimators: It controls the number of weak learners.\n",
    "- Learning Rate: Controls the contribution of weak learners in the final combination. There is a trade-off between learning_rate and n_estimators.\n",
    "- Loss Reduction: The loss function to use during splitting\n",
    "- Sample size: The proportion of the data exposed to the model during each iteration.\n",
    "- Max_depth: maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463bf9f-c3d3-4313-8f33-9cd7774e0e0e",
   "metadata": {},
   "source": [
    "#### XG Boosting <a class=\"anchor\" id=\"section_3_2_3\"></a>\n",
    "\n",
    "- One of the most widely used algorithms for gradient boosting is XGBoost which stands for “extreme gradient boosting” which is the a more advanced version of the gradient boosting method. XGboost as well as other gradient boosting methods has many parameters to regularize and optimize the complexity of the model. This flexibility comes with benefits; methods depending on XGboost have won many machine learning competitions.\n",
    "\n",
    "- XGBoost was introduced because the gradient boosting algorithm was computing the output at a prolonged rate right because there's a sequential analysis of the data set and it takes a longer time.\n",
    "\n",
    "- The main aim of this algorithm is to increase **speed** and model **performance**.\n",
    "\n",
    "- XGBoost is also called a regularized boosting technique. This helps to reduce overfit modelling.\n",
    "\n",
    "- It supports **parallelization** by creating decision trees. There's no sequential modeling in computing methods for evaluating any large and any complex modules.\n",
    "\n",
    "- Around the time it was first invented and published, XGBoost used to be the algorithm-of-choice for Kagglers. Lots of the winners in around the years 2015 and 2016 won their prize using XGBoost.\n",
    "\n",
    "\n",
    "XGBoost is similar to gradient boosting algorithm but it has a few tricks up its sleeve which makes it stand out from the rest.\n",
    "\n",
    "Features of XGBoost are:\n",
    "\n",
    "- Clever Penalisation of Trees\n",
    "- A Proportional shrinking of leaf nodes\n",
    "- Newton Boosting\n",
    "- Extra Randomisation Parameter\n",
    "\n",
    "In XGBoost the trees can have a varying number of terminal nodes and left weights of the trees that are calculated with less evidence is shrunk more heavily. Newton Boosting uses Newton-Raphson method of approximations which provides a direct route to the minima than gradient descent. The extra randomisation parameter can be used to reduce the correlation between the trees,  the lesser the correlation among classifiers, the better our ensemble of classifiers will turn out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470fb0bf-264a-4421-abd6-fe7f3a654bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier(n_estimators = 100, max_depth = 3)\n",
    "clf.fit(x_train,y_train)\n",
    "clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f8231-18be-4d82-b911-f16a9a8f7672",
   "metadata": {},
   "source": [
    "### 3 Stacking <a class=\"anchor\" id=\"section_4_4_1\"></a>\n",
    "\n",
    "**Stacking:**  Trains N different weak models (usually of different types – heterogenous) with one of the two subsets of the\n",
    "dataset in parallel. Once the weak learners are trained, they are used to trained a meta learner to combine their predictions and carry out final prediction using the other subset. In test phase, each model predicts its label, these set of labels are fed to the meta learner which generates the final prediction.\n",
    "\n",
    "<img src=\"Images/stack.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a962d-4f4a-473b-9564-2406b72f5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "models = [ ('lr', LinearRegression()),('dt', DecisionTreeRegressor()]\n",
    "stacking = StackingRegressor(estimators=models, final_estimator=RandomForestRegressor(n_estimators=10,random_state=42))\n",
    "stacking.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde21643-8cdc-4952-a558-df63f6e12ef5",
   "metadata": {},
   "source": [
    "**Which is the best, Bagging or Boosting?**\n",
    "There’s not an outright winner; it depends on the data, the simulation and the circumstances.\n",
    "\n",
    "- Bagging and Boosting decrease the variance of your single estimate as they combine several estimates from different models. So the result may be a model with higher stability.\n",
    "\n",
    "- Both are good at reducing variance and provide higher stability but only Boosting tries to reduce bias. On the other hand, Bagging may solve the over-fitting problem, while Boosting can increase it.\n",
    "\n",
    "- If the problem is that the single model gets a **very low performance**, Bagging will rarely get a better bias. However, Boosting could generate a combined model with lower errors as it optimises the advantages and reduces pitfalls of the single model.\n",
    "\n",
    "- By contrast, if the difficulty of the single model is **over-fitting**, then Bagging is the best option. Boosting for its part doesn’t help to avoid over-fitting; in fact, this technique is faced with this problem itself. For this reason, Bagging is effective more often than Boosting.\n",
    "\n",
    "<img src=\"Images/bbs.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db9eac-46e1-4154-883a-4a55cf2852ea",
   "metadata": {},
   "source": [
    "## PART 2 <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178d242-2425-416f-b3c5-c310dfd6de75",
   "metadata": {},
   "source": [
    "## Cross Validation <a class=\"anchor\" id=\"section_4_1\"></a>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd3e3fa1-092e-4309-8c2d-7b75d1cdf143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size :  (569, 30) (569,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "df = datasets.load_breast_cancer()\n",
    "X_df, Y_df = df.data, df.target\n",
    "print('Dataset Size : ', X_df.shape, Y_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5f4c7-796c-46b0-9588-88528edfc624",
   "metadata": {},
   "source": [
    "#### Simple Holdout method or Train-test split\n",
    "\n",
    "In Train-Test split method, the data is first shuffled randomly before splitting. As the model is trained on a different combination of data points, the model can give different results every time we train it, and this can be a cause of instability. \n",
    "\n",
    "Also, we can never assure that the train set we picked is representative of the whole dataset. Also when our dataset is not too large, there is a high possibility that the testing data may contain some important information that we lose as we do not train the model on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "823a3563-9796-48a5-87a3-a0eeeeacfdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test Sizes :  (455, 30) (114, 30) (455,) (114,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, train_size=0.80, test_size=0.2, random_state=1)\n",
    "print('Train/Test Sizes : ',X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98e0552-5981-4a4c-9ae4-aaa20e250e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.95\n",
      "Test Accuracy : 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier().fit(X_train, Y_train)\n",
    "print('Train Accuracy : %.2f'%knn.score(X_train, Y_train))\n",
    "print('Test Accuracy : %.2f'%knn.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce236e13-5326-4782-9131-72cb3be88ee4",
   "metadata": {},
   "source": [
    "### **Cross-validation** \n",
    "Cross-Validation is a statistical method of evaluating and comparing learning algorithms by dividing data into two segments: one used to train a model and the other used to validate the model on different iterations.\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/kituDjzXwfE/maxresdefault.jpg\" width=\"600\"/> <br>\n",
    "\n",
    "\n",
    "<font color='Brown'>**Why CV:**</font>\n",
    "\n",
    "1. **More “efficient” use of data**- In this method it matters less how the data gets divided, as every observation is used for both training and testing\n",
    "\n",
    "2. **Test the ability of a machine learning model to predict new data** - Every data point gets to be in a test set exactly once, and gets to be in a training set k-1 times.\n",
    "\n",
    "3. **Flag problems like overfitting or selection bias** - by not generalizing a pattern.\n",
    "\n",
    "4. **Parameters Fine-Tuning** - Most of the learning algorithms require some parameters tuning. We do it by trying different values and choosing the best ones. The same data cannot be used for both training and hyperpameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204d0a7-713d-4bf0-bf55-f46d879f3323",
   "metadata": {},
   "source": [
    "\n",
    "#### <font color='Brown'>**Types of CVs:**</font>\n",
    "\n",
    "1. K-Fold Cross Validation\n",
    "2. Stratified K-fold Cross-Validation - Stratification is the process of rearranging the data so as to ensure that each fold is a good representative of the whole.\n",
    "3. Leave One-out Cross Validation (LOOCV)\n",
    "4. Leave P-out Cross Validation (LPOCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd7002-c06e-49f6-b4b8-4b060f437bf5",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "<font color='Brown'>**How it works:**</font>\n",
    "\n",
    "\n",
    "1. Shuffle the dataset randomly.\n",
    "2. Split the training dataset into k groups\n",
    "3. For each unique group:\n",
    "    - Take 1 group as a hold out or validation data set\n",
    "    - Take the remaining groups as a training data set\n",
    "    - Fit a model on the training set and evaluate it on the validation set\n",
    "    - Retain the evaluation score and discard the model\n",
    "4. Repeat steps 3 *K* times. Each time use the remaining  fold as the test set.\n",
    "5. Summarize the evaluation done on each fold\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/0SQJq.png\" width=\"800\"/> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94d0ef7-ad8b-4f02-9084-beff43701b7d",
   "metadata": {},
   "source": [
    "#### Cross Validation - K fold & Stratified K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f47cc093-8360-401a-b5f6-837d17d950e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc2903f-5687-4237-aa12-37b35cb0fc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying With KFold Cross Validation :  [0.89010989 0.87912088 0.91208791 0.96703297 0.91208791]\n",
      "Final score With KFold Cross Validation :  0.9120879120879121\n"
     ]
    }
   ],
   "source": [
    "#### K fold Cross Validation\n",
    "\n",
    "kcv = cross_val_score(KNeighborsClassifier(), X_train, Y_train, cv=KFold(n_splits=5))\n",
    "\n",
    "print('Classifying With KFold Cross Validation : ', kcv)\n",
    "print('Final score With KFold Cross Validation : ', kcv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7814a42a-01ef-443c-8425-0df0bd603e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without mention of Cross Validation technique :  [0.87912088 0.89010989 0.91208791 0.96703297 0.91208791]\n",
      "Classifying With Stratified KFold Cross Validation :  [0.87912088 0.89010989 0.91208791 0.96703297 0.91208791]\n",
      "Final score With Stratified KFold Cross Validation :  0.9120879120879121\n"
     ]
    }
   ],
   "source": [
    "#### Stratified K fold Cross Validation\n",
    "\n",
    "skcv = cross_val_score(KNeighborsClassifier(), X_train, Y_train, cv=5)\n",
    "skcv1 = cross_val_score(KNeighborsClassifier(), X_train, Y_train, cv=StratifiedKFold(n_splits=5))\n",
    "\n",
    "print('Without mention of Cross Validation technique : ', skcv) ## It uses StratifiedKFold default\n",
    "print('Classifying With Stratified KFold Cross Validation : ', skcv1)\n",
    "print('Final score With Stratified KFold Cross Validation : ', skcv1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaad7cf2-a168-4dc0-b286-04b338c89e36",
   "metadata": {},
   "source": [
    "##### Trade-offs Between Cross-Validation and Train-Test Split\n",
    "\n",
    "1. Size of dataset \n",
    "2. Computational time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aaee2f-5d75-47ee-bc58-b8f850ac5877",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning / Optimization <a class=\"anchor\" id=\"section_5_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86696784-14b3-44d7-839a-dc08c5e1a13e",
   "metadata": {},
   "source": [
    "**Definition:**  Hyperparameter optimization, also called hyperparameter tuning, is the process of searching for a set of hyperparameters that gives the best model results on a given dataset.\n",
    "\n",
    "Hyperparameters are parameters that are defined before training to specify how we want model training to happen\n",
    "    Example: In random forest model, n_estimators (number of decision trees we want to have) is a hyperparameter. It can be set to any integer value but of course, setting it to 10 or 1000 changes the learning process significantly.\n",
    "\n",
    "\n",
    "**Model parameters:** A model parameter is a configuration variable that is internal to the model and whose value can be estimated from data.They are required by the model when making predictions and are estimated or learnt from data.\n",
    "    Example: Coefficients in Linear/Logistic regression, Split points in Decision Tree <br>\n",
    "    \n",
    "**Model hyperparameters:** These are adjustable parameters that can be tuned in order to obtain a model with optimal performance. These parameters express “higher-level” properties of the model such as its complexity or how fast it should learn. \n",
    "    Example: k in k-nearest neighbors, depth of a tree\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/875/1*FIIGhzbuTo2vI62mFcbMTg.png\" width=\"350\"/> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82d637-2d6e-4f18-8a22-6269955a0025",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Types of Hyperparameter tuning:**</font>  \n",
    "\n",
    "##### 1. **Manual method:** \n",
    "Select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results.\n",
    "    Example: Changing lambda values in Ridge Regression, K values in KNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136c27a-ac54-4add-8a3b-a1b2d1b3032b",
   "metadata": {},
   "source": [
    "#### 1. GridSearchCV <a class=\"anchor\" id=\"section_5_1_1\"></a>\n",
    "\n",
    "\n",
    "In the grid search method, we create a grid of possible values for hyperparameters. Each iteration tries a combination of hyperparameters in a specific order. It fits the model on each combination of hyperparameters possible and records the model performance. Finally, it returns the best model with the best hyperparameters.\n",
    "\n",
    "\n",
    "P1 = [v1, v2] <br>\n",
    "P2 = [b1, b2]\n",
    "\n",
    "Combinations --> v1b1, v1by, v2b1, v2b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724d77d2-20f9-43c5-b8fc-0c512a0d51eb",
   "metadata": {},
   "source": [
    "**GridSearchCV (estimator, param_grid, scoring=None, n_jobs=None, cv=None)**\n",
    "\n",
    "- *param_grid* -->  Dictionary with parameters names (str) as keys and distributions or lists of parameters to try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71435d4-2c98-4181-866e-c1cc2b204db4",
   "metadata": {},
   "source": [
    "<img src=\"https://static.wixstatic.com/media/fd32e3_50364e42770b42c28e3d9837487e12d1~mv2.png/v1/fill/w_612,h_236,al_c/fd32e3_50364e42770b42c28e3d9837487e12d1~mv2.png\" width=\"700\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25c0db-8102-4b6b-9ce2-80a1a46e80be",
   "metadata": {},
   "source": [
    "#### 2. RandomsearchCV <a class=\"anchor\" id=\"section_5_1_2\"></a>\n",
    "\n",
    "In the random search method, we create a grid of possible values for hyperparameters. Each iteration tries a random combination of hyperparameters from this grid, records the performance, and lastly returns the combination of hyperparameters that provided the best performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38969b0-d1e5-4dc4-865f-f6a858c57839",
   "metadata": {},
   "source": [
    "**RandomizedSearchCV(estimator, param_distributions, n_iter=10, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, random_state=None, return_train_score=False)**\n",
    "\n",
    "- *param_distributions* -->  Dictionary with parameters names (str) as keys and distributions or lists of parameters to try.\n",
    "- *cv:*  -->  Determines the cross-validation splitting strategy ((Stratified)KFold - default 5-fold)\n",
    "- *scoring:* --> Evaluation metrics (default: accuracy_score for Classification & r2_score for Regression)\n",
    "- *n_iter:* --> Number of parameter settings that are sampled (default=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be106e-1894-4ea3-8a00-ddf42d6eb812",
   "metadata": {},
   "source": [
    "<font color='DarkBlue'>**Steps of HyperParameter Tuning**</font> \n",
    "\n",
    "1. Select the right type of estimator.\n",
    "2. Review the list of parameters of the model and build the HP space\n",
    "3. Finding the methods for searching the hyperparameter space\n",
    "4. Applying the cross-validation scheme approach\n",
    "5. Assess the model score to evaluate the model\n",
    "\n",
    "<font color='DarkBlue'>**Hyperparameters Nature:**</font> \n",
    "\n",
    "    Discrete: Number of estimators in ensemble models. E.g. 'n_estimators' : [50, 100]\n",
    "    Continuous: Penalization coefficient, Number of samples per split. E.g. 'max_depth': list(range(2, 10))}\n",
    "    Categorical: Loss (deviance, exponential), Regularization (Lasso, Ridge). E.g. 'criterion': ['gini','entropy']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6a979-ba7f-4722-9b0f-17fe98847c13",
   "metadata": {},
   "source": [
    "#### <font color='Brown'>**Hyperparameter tuning for Random Forest example**</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "715dd3d7-d25c-445c-8666-8344d9f3e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "## glass.csv from Kaggle as Input dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "#print(data.DESCR) \n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# split the data using Scikit-Learn's train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4abe00c-4acd-4ed3-ad72-8effe494c429",
   "metadata": {},
   "source": [
    "##### <font color='Darkblue'>**Basic Random forest model without tuning**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4963efb9-5cd4-409c-8cb6-e62b8c7fbe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score without tuning: 0.958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and call accuracy\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print('Accuracy score without tuning:' , round(accuracy_score(y_pred,y_test), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a8cb9a0-487a-4b81-9709-97ea03c7ac40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad062507-ab54-4fd6-a4ee-95db35b635d8",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Parameters:**</font>  \n",
    "\n",
    "**Important hyperparameters**\n",
    "    \n",
    "    n_estimators: Number of decision trees\n",
    "    max_features: Maximum number of features considered while splitting\n",
    "    max_depth: Max depth of the tree\n",
    "    min_samples_leaf: Minimum number of data points in a leaf node\n",
    "    min_samples_split: min number of data points placed in a node before the node is split\n",
    "    \n",
    " \n",
    "**Hyperparameters that do not affect model performance**\n",
    "\n",
    "    verbose: Printing information while training continues (the higher, the more messages)\n",
    "    n_jobs: Number of jobs to run in parallel (default=1)\n",
    "    random_state: Seed\n",
    " \n",
    " \n",
    " <font color='Brown'>**Attributes:**</font>  \n",
    "\n",
    "- **get_params** -->  Get default parameters for the estimator/model.\n",
    "\n",
    "- **best_estimator_** -->  Estimator which gave highest score (or smallest loss if specified) on the left out data\n",
    "\n",
    "- **best_params_** -->  Parameter setting that gave the best results on the hold out data. \n",
    "\n",
    "- **cv_results_** --> Returns a dictionary of all the evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f03b8-3ff3-4921-9725-604a588b456a",
   "metadata": {},
   "source": [
    "##### <font color='Darkblue'>**Model tuning with GridSearchCV**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb11da73-83ef-4f1f-9985-108dd3d3554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid\n",
    "param_grid = {\n",
    "'n_estimators': [50, 100, 200],\n",
    "'min_samples_leaf': [2, 5, 10],\n",
    "'criterion': ['gini','entropy'],\n",
    "'max_depth': [2, 3, 5, 8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1da0adc-edb1-4dc8-9d36-3e21b496d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_gridsearch = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='accuracy',  cv=5, refit = True, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e835822-5d57-4144-8e75-308340abe40b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 58.09 seconds for 72 candidate parameter settings.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate GridSearchCV\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "model_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Print the time spend and number of models ran\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\" % ((time.time() - start), len(model_gridsearch.cv_results_['params'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "831de45d-9282-4fe6-b38c-ec25cb6e71a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on the test set and call accuracy\n",
    "y_pred_grid = model_gridsearch.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec2bbf48-c7f2-42b2-9442-e3cb60ad38aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 5,\n",
       " 'min_samples_leaf': 2,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c75c2ba-76f0-4078-92ad-7555ae7a5b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=5, min_samples_leaf=2, n_estimators=200,\n",
       "                       random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=5, min_samples_leaf=2, n_estimators=200,\n",
       "                       random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=5, min_samples_leaf=2, n_estimators=200,\n",
       "                       random_state=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b87b0-dd81-473b-a8a8-d84609cae096",
   "metadata": {},
   "source": [
    "##### <font color='Darkblue'>**Model tuning with RandomSearchCV**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c1dbe18-d925-41cf-9e67-f6f8aec23891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "'n_estimators': list(range(50, 200, 10)),\n",
    "'min_samples_split': list(range(2, 10)),    \n",
    "'criterion': ['gini','entropy'],\n",
    "'max_depth': list(range(2, 8))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f4d21a6-61d5-4650-8b5c-45bb906bdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model_random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist,  cv = 5, refit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c363d7c-6f45-41da-90c3-22b0e3eccb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 8.81 seconds for 10 candidate parameter settings.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate RandomizedSearchCV\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model_random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the time spend and number of models ran\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidate parameter settings.\" % ((time.time() - start), len(model_random_search.cv_results_['params'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba377756-9673-4772-83da-17f93ccb196b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 120,\n",
       " 'min_samples_split': 2,\n",
       " 'max_depth': 6,\n",
       " 'criterion': 'gini'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71478cc1-02e1-43fd-9992-c8143458e925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on the test set and call accuracy\n",
    "y_pred_random = model_random_search.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2def625b-33b7-46be-ada5-1ec93890f7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_estimators': 160,\n",
       "  'min_samples_split': 9,\n",
       "  'max_depth': 2,\n",
       "  'criterion': 'gini'},\n",
       " {'n_estimators': 180,\n",
       "  'min_samples_split': 8,\n",
       "  'max_depth': 4,\n",
       "  'criterion': 'gini'},\n",
       " {'n_estimators': 90,\n",
       "  'min_samples_split': 9,\n",
       "  'max_depth': 6,\n",
       "  'criterion': 'entropy'},\n",
       " {'n_estimators': 110,\n",
       "  'min_samples_split': 5,\n",
       "  'max_depth': 6,\n",
       "  'criterion': 'gini'},\n",
       " {'n_estimators': 80,\n",
       "  'min_samples_split': 6,\n",
       "  'max_depth': 2,\n",
       "  'criterion': 'gini'},\n",
       " {'n_estimators': 120,\n",
       "  'min_samples_split': 2,\n",
       "  'max_depth': 6,\n",
       "  'criterion': 'gini'},\n",
       " {'n_estimators': 150,\n",
       "  'min_samples_split': 7,\n",
       "  'max_depth': 7,\n",
       "  'criterion': 'gini'},\n",
       " {'n_estimators': 190,\n",
       "  'min_samples_split': 6,\n",
       "  'max_depth': 2,\n",
       "  'criterion': 'gini'},\n",
       " {'n_estimators': 80,\n",
       "  'min_samples_split': 6,\n",
       "  'max_depth': 5,\n",
       "  'criterion': 'entropy'},\n",
       " {'n_estimators': 60,\n",
       "  'min_samples_split': 8,\n",
       "  'max_depth': 6,\n",
       "  'criterion': 'entropy'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#search_RF.cv_results_.\n",
    "\n",
    "model_random_search.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43157b19-e8b6-4a55-a33a-fe85b25e17e2",
   "metadata": {},
   "source": [
    "##### <font color='Brown'>**Hyperparameters for other algorithm** </font>\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "    Regularization :  penalty in [‘none’, ‘l1’, ‘l2’, ‘elasticnet’]\n",
    "    Penality strength : C in [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "**K-Nearest Neighbors (KNN)**\n",
    "\n",
    "    Number of neighbors : n_neighbors in [1 to 21]\n",
    "    Distance metrics : metric in [‘euclidean’, ‘manhattan’, ‘minkowski’]\n",
    "    \n",
    "**Gradient Boosting**\n",
    "\n",
    "    learning_rate in [0.001, 0.01, 0.1]\n",
    "    Number of trees in the model : n_estimators [10, 100, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8c8764-bf40-4a55-aad1-f6ca16409786",
   "metadata": {},
   "source": [
    "## Model / Algorithm Selection<a class=\"anchor\" id=\"section_6_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ab2dc-ffe6-4396-9293-f11425cf6db4",
   "metadata": {},
   "source": [
    "**Learning Algorithm Selection:**\n",
    "Choosing the right ML algorithm for your task can be overwhelming. There are dozens of options, each with their own advantages and disadvantages.\n",
    "\n",
    "1. Based on business case and a solid understanding of what you are trying to accomplish.\n",
    "\n",
    "2. Identify the type of Problem - Regression / Classification\n",
    "\n",
    "3. Data understanding - Type of relationship, size of data & dimensions, data quality\n",
    "\n",
    "4. Model & Time complexity - Training and Prediction time against resource available, Amount of parameter tuning needed\n",
    "\n",
    "5. Model interpretability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7da9d1-2b04-4571-b9a5-266600d15891",
   "metadata": {},
   "source": [
    "<img src=\"Images/Comparison.PNG\" width=\"1800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e538ce7-03d9-4a74-aa49-412a721062d5",
   "metadata": {},
   "source": [
    "**Model Selection:** is the process of choosing one of the models as the final model that addresses the problem.\n",
    "\n",
    "1. Performance of Model accross various Evaluation metrics\n",
    "\n",
    "2. Time & Resouces used - For training, Prediction and tuning Hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
