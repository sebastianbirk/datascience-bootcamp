{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d6d8c6-e542-4ebe-954c-024c85d116f0",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning Models - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c4bf7-2dca-471a-a867-ac36da82fa77",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [PART 1](#chapter1)  \n",
    "\n",
    "    * [Ensemble Learning algorithms](#section_1_1)\n",
    "        1. [Bagging algorithms](#Section_1_1_1)\n",
    "            *  [Random Forest](#section_2_1_1)\n",
    "        3. [Boosting algorithms](#section_3_1_1)\n",
    "             * [AdaBoost](#section_3_2_1)\n",
    "             * [Gradient Boosting XGBoost ](#section_3_2_2)\n",
    "             * [XGBoost ](#section_3_2_3)\n",
    "        4. [Stacking](#section_4_1_1)\n",
    "<br>\n",
    "\n",
    "* [PART 2](#chapter2)\n",
    "\n",
    "    * [Cross Validation](#section_4_1)\n",
    "    * [HyperParameter Tuning](#section_5_1)\n",
    "        * [GridsearchCV](#section_5_1_1)\n",
    "        * [RandomSearchCV](#section_5_1_2)\n",
    "    * [Model Comparison](#section_6_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8af364-9270-4ba7-8215-825c0717ef19",
   "metadata": {},
   "source": [
    "## PART 1 <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fcb1dc-50f9-4f24-b1bc-d842f57d0c8b",
   "metadata": {},
   "source": [
    "## Ensemble Learning algorithms <a class=\"anchor\" id=\"section_1_1\"></a>\n",
    "\n",
    "Ensemble learning models are simply combinations of different machine learning models.Instead of training one large/complex model for your dataset, you train multiple small/simpler models (weak-learners) and aggregate their output (in various ways) to form your prediction.\n",
    "\n",
    "It combines multiple weak models/learners into one predictive model to reduce bias, variance and/or improve accuracy.\n",
    "\n",
    "\n",
    "<font color='Brown'>**Types of Ensemble Learning: N number of weak learners:**</font>\n",
    "1. Bagging\n",
    "2. Boosting\n",
    "3. Stacking\n",
    "\n",
    "These methods have the same wisdom-of-the-crowd concept but differ in the details of what it focuses on, the type of weak learners used, and the type of aggregation used to form the final output.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b91607-0f95-4aa7-88db-ab953213ebfd",
   "metadata": {},
   "source": [
    "### 1. Bagging algorithms <a class=\"anchor\" id=\"Section_1_1_1\"></a>\n",
    "\n",
    "**Bagging (Boostrap Aggregating):** Trains N different weak models (usually of same types – homogenous) with N non-overlapping subset of the input dataset **in parallel.** In the test phase, each model is evaluated. The label with the greatest number of predictions is selected as the prediction. Bagging methods reduces variance of the prediction.\n",
    "\n",
    "For each weak-learner, the input data is randomly sampled from the original dataset with replacement and is trained. By sampling with replacement some observations may be repeated in each new training data set. A random sampling of the subset with replacement creates nearly iid samples. During inference, the test input is fed to all the weak-learners and the output is collected. The result outputted from bagging is the average (if the problem is regression) or the most suitable label by the voting scheme (if the problem is classification).\n",
    "\n",
    "In bagging methods, the weak-learners, usually are of the same type. Since the random sampling with replacement creates iid samples, and aggregating iid variables doesn’t change the bias but reduces variance, the bagging method doesn't change the bias in the prediction but **reduces its variance.**\n",
    "\n",
    "<img src=\"Images/bag.png\" width=\"300\"/>\n",
    "\n",
    "Each decision tree in the bag is trained on an independent subset of the training data. These subsets are random bootstraps of the whole training set. In other words, suppose the training data is a table with n observations on m features. Each component tree of the bagging will receive a subset of k observations on m features to train on, with k < n. Each observation of a subset is drawn from the full data with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a099f76e-6e62-46f1-8a2e-a011a8813b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "bag = BaggingRegressor(base_estimator=DecisionTreeRegressor()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf87c9a8-87ec-4fce-be22-3bb85c871cc4",
   "metadata": {},
   "source": [
    "#### Random Forest <a class=\"anchor\" id=\"section_2_1_1\"> </a>\n",
    "\n",
    "- Random forest is very similar to bagging: it also consists of many decision trees, each of the trees is assigned with a bootstrap sample of the training data, and the final result of the meta-model is computed as the average or mode of the outputs from the components.\n",
    "\n",
    "- The only difference is that random forests, when splitting a node of a component tree, not all of the features are taken as candidates to split. Instead, only a subset of the whole feature set is selected to be the candidates (the selection is random for each node) and then the best feature from this subset is appointed to be the splitting test at that node. Suppose there are m features overall, the size of the subset can be any number from 1 to m-1.\n",
    "\n",
    "- The random forests algorithm tries to decorrelate the trees so that they learn different things about the data. It does this by selecting **a random subset of variables.** If one or a few independent variables are very strong predictors for the response variable, these features will be selected in many of the trees, causing them to become correlated. Random subsampling of independent variables ensures that not always the best predictors overall are selected for every tree and, the model does have a chance to learn other features of the data.\n",
    "\n",
    "- They are simply ensembles of decision trees. Each tree is trained with a different randomly selected part of the data with randomly selected independent variables. The goal of introducing randomness is **to reduce the variance** of the model so it does **not overfit**, at the expense of a **small increase in the bias** and **some loss of interpretability**. This strategy generally boosts the performance of the final model.\n",
    "\n",
    "**How to:**\n",
    "\n",
    "- Individual trees are built independently, using the same procedure as for a normal decision tree but with only a random portion of the data and only considering a random subset of the features at each node. Aside from this, the training procedure is exactly the same as for an individual Decision Tree, repeated N times.\n",
    "\n",
    "- To make a prediction using a Random Forest each an individual prediction is obtained from each tree. Then, if it is a classification problem, we take the most frequent prediction as the result, and if it is a regression problem we take the average prediction from all the individual trees as the output value. The following figure illustrates how this is done:\n",
    "\n",
    "\n",
    "<img src=\"Images/randomforest.png\" width=\"700\"/>\n",
    "\n",
    "**Parameters:**\n",
    "For random forests, we have two critical arguments. \n",
    "\n",
    "- Number of Features: how many columns to use when sampling \n",
    "- Number of Trees: how many trees to average\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42d15c-3ea8-4686-86b3-1624c26fbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr= RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382523b7-955f-4526-b640-4307bf4149be",
   "metadata": {},
   "source": [
    "### 2. Boosting algorithms <a class=\"anchor\" id=\"section_3_1_1\"></a>\n",
    "\n",
    "The general idea of boosting also encompasses building multiple weak learners to contribute to the final result. However, these component trees are built **sequentially**, one after another, and how to build the latter one is dependent on the result of the formers. Put another way, the next weak learner is built in a way to specifically improve on what the existing weak learners are bad at.\n",
    "\n",
    "There are a few answers to how should the next tree address the shortcomings of the previous trees, these answers divide boosting into several styles. The most popular styles are **AdaBoost**, **Gradient Boosting** and **XGBoost**\n",
    "\n",
    "In the test phase, each model is evaluated and based on the test error of each weak model, the prediction is weighted for voting. Boosting methods **decreases the bias of the prediction.**\n",
    "\n",
    "<img src=\"Images/boost.png\" width=\"300\"/>\n",
    "\n",
    "Step 1:  The base learner takes all the distributions and assign equal weight or attention to each observation.\n",
    "\n",
    "Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n",
    "\n",
    "Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n",
    "\n",
    "Finally, it combines the outputs from weak learner and creates  a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b72445-7c28-472a-8aee-e74284b55365",
   "metadata": {},
   "source": [
    "#### AdaBoost (Adaptive Boosting) <a class=\"anchor\" id=\"section_3_2_1\"></a>\n",
    "\n",
    "Adaptive Boosting aka AdaBoost fits a sequence of weak learners on different weighted training data. It starts by predicting original data set and gives equal weight to each observation. If prediction is incorrect using the first learner, then it gives higher weight to observation which have been predicted incorrectly. Being an iterative process, it continues to add learner(s) until a limit is reached in the number of models or accuracy.\n",
    "\n",
    "**How:**\n",
    "<img src=\"Images/ada.png\" width=\"500\"/>\n",
    "\n",
    "***Box 1:*** You can see that we have assigned equal weights to each data point and applied a decision stump to classify them as + (plus) or – (minus). The decision stump (D1) has generated vertical line at left side to classify the data points. We see that, this vertical line has incorrectly predicted three + (plus) as – (minus). In such case, we’ll assign higher weights to these three + (plus) and apply another decision stump.\n",
    "\n",
    "***Box 2:*** Here, you can see that the size of three incorrectly predicted + (plus) is bigger as compared to rest of the data points. In this case, the second decision stump (D2) will try to predict them correctly. Now, a vertical line (D2) at right side of this box has classified three mis-classified + (plus) correctly. But again, it has caused mis-classification errors. This time with three -(minus). Again, we will assign higher weight to three – (minus) and apply another decision stump.\n",
    "\n",
    "***Box 3:*** Here, three – (minus) are given higher weights. A decision stump (D3) is applied to predict these mis-classified observation correctly. This time a horizontal line is generated to classify + (plus) and – (minus) based on higher weight of mis-classified observation.\n",
    "\n",
    "***Box 4:*** Here, we have combined D1, D2 and D3 to form a strong prediction having complex rule as compared to individual weak learner. You can see that this algorithm has classified these observation quite well as compared to any of individual weak learner.\n",
    "\n",
    "We can use AdaBoost algorithms for both classification and regression problem.\n",
    "\n",
    "***The drawback of AdaBoost*** is that it is easily defeated by noisy data, the efficiency of the algorithm is highly affected by outliers as the algorithm tries to fit every point perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259dedf-e47e-46cf-9f28-818ecf5db4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier #For Classification\n",
    "from sklearn.ensemble import AdaBoostRegressor #For Regression\n",
    "clf = AdaBoostClassifier(n_estimators = 50, base_estimator = DecisionTreeClassifier)\n",
    "clf.fit(x_train,y_train)\n",
    "clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc42f30-26bc-4d1a-a013-2c6335365b49",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "- N estimators: It controls the number of weak learners.\n",
    "- Learning Rate: Controls the contribution of weak learners in the final combination. There is a trade-off between learning_rate and n_estimators.\n",
    "- Base estimators: It helps to specify different ML algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4467e4f7-5503-4d26-910d-be4405e18ec0",
   "metadata": {},
   "source": [
    "#### Gradient Boosting <a class=\"anchor\" id=\"section_3_2_2\"></a>\n",
    "\n",
    "Gradient boosting is also based on the sequential and symbol learning model. The base learners are generated sequentially in such a way that the present based learner is always more effective than the previous one. The overall model improves sequentially with each iteration now.\n",
    "\n",
    "However, in this boosting the weights for misclassified outcomes are not incremented. The main idea here is to overcome the residual errors (prediction error for the current ensemble of models) of the previous model. It tries to optimize the loss function of previous learner by adding a new adaptive model that combines weak learners. Specifically in gradient boosting, the simple models are trees. As in random forests, many trees are grown but in this case, trees are sequentially grown and each tree focuses on fixing the shortcomings of the previous trees. \n",
    "\n",
    "<img src=\"Images/gb.png\" width=\"500\"/>\n",
    "\n",
    "Gradient boosting is a greedy algorithm and can overfit a training dataset quickly.\n",
    "\n",
    "The loss function is the one that needs to be optimized (Reduce the error) You have to keep adding a model that will regularize the loss function from the previous learner.\n",
    "Just like adaptive boosting gradient boosting can also be used for both classification and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d276241-d681-450d-8199-5b0826595011",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradientBoostingRegressor \u001b[38;5;66;03m#For Regression\u001b[39;00m\n\u001b[1;32m      3\u001b[0m clf \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier #For Classification\n",
    "from sklearn.ensemble import GradientBoostingRegressor #For Regression\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, loss_function = deviance, max_depth=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0621683f-503f-4e9d-8af1-083cd8ff5f2a",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "- N estimators: It controls the number of weak learners.\n",
    "- Learning Rate: Controls the contribution of weak learners in the final combination. There is a trade-off between learning_rate and n_estimators.\n",
    "- Loss Reduction: The loss function to use during splitting\n",
    "- Sample size: The proportion of the data exposed to the model during each iteration.\n",
    "- Max_depth: maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463bf9f-c3d3-4313-8f33-9cd7774e0e0e",
   "metadata": {},
   "source": [
    "#### XG Boosting <a class=\"anchor\" id=\"section_3_2_3\"></a>\n",
    "\n",
    "- One of the most widely used algorithms for gradient boosting is XGBoost which stands for “extreme gradient boosting” which is the a more advanced version of the gradient boosting method. XGboost as well as other gradient boosting methods has many parameters to regularize and optimize the complexity of the model. This flexibility comes with benefits; methods depending on XGboost have won many machine learning competitions.\n",
    "\n",
    "- XGBoost was introduced because the gradient boosting algorithm was computing the output at a prolonged rate right because there's a sequential analysis of the data set and it takes a longer time.\n",
    "\n",
    "- The main aim of this algorithm is to increase **speed** and model **performance**.\n",
    "- It supports **parallelization** by creating decision trees. There's no sequential modeling in computing methods for evaluating any large and any complex modules.\n",
    "\n",
    "- Around the time it was first invented and published, XGBoost used to be the algorithm-of-choice for Kagglers. Lots of the winners in around the years 2015 and 2016 won their prize using XGBoost.\n",
    "\n",
    "The support includes various objective functions, including regression, classification and ranking.\n",
    "\n",
    "One of the most interesting things about the XGBoost is that it is also called a regularized boosting technique. This helps to reduce overfit modelling.\n",
    "\n",
    "XGBoost is similar to gradient boosting algorithm but it has a few tricks up its sleeve which makes it stand out from the rest.\n",
    "\n",
    "Features of XGBoost are:\n",
    "\n",
    "- Clever Penalisation of Trees\n",
    "- A Proportional shrinking of leaf nodes\n",
    "- Newton Boosting\n",
    "- Extra Randomisation Parameter\n",
    "\n",
    "In XGBoost the trees can have a varying number of terminal nodes and left weights of the trees that are calculated with less evidence is shrunk more heavily. Newton Boosting uses Newton-Raphson method of approximations which provides a direct route to the minima than gradient descent. The extra randomisation parameter can be used to reduce the correlation between the trees,  the lesser the correlation among classifiers, the better our ensemble of classifiers will turn out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470fb0bf-264a-4421-abd6-fe7f3a654bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier(n_estimators = 100, max_depth = 3)\n",
    "clf.fit(x_train,y_train)\n",
    "clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde21643-8cdc-4952-a558-df63f6e12ef5",
   "metadata": {},
   "source": [
    "**Which is the best, Bagging or Boosting?**\n",
    "There’s not an outright winner; it depends on the data, the simulation and the circumstances.\n",
    "Bagging and Boosting decrease the variance of your single estimate as they combine several estimates from different models. So the result may be a model with higher stability.\n",
    "\n",
    "Both are good at reducing variance and provide higher stability but only Boosting tries to reduce bias. On the other hand, Bagging may solve the over-fitting problem, while Boosting can increase it.\n",
    "\n",
    "If the problem is that the single model gets a very low performance, Bagging will rarely get a better bias. However, Boosting could generate a combined model with lower errors as it optimises the advantages and reduces pitfalls of the single model.\n",
    "\n",
    "By contrast, if the difficulty of the single model is over-fitting, then Bagging is the best option. Boosting for its part doesn’t help to avoid over-fitting; in fact, this technique is faced with this problem itself. For this reason, Bagging is effective more often than Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f8231-18be-4d82-b911-f16a9a8f7672",
   "metadata": {},
   "source": [
    "### 3 Stacking <a class=\"anchor\" id=\"section_4_4_1\"></a>\n",
    "\n",
    "**Stacking:**  Trains N different weak models (usually of different types – heterogenous) with one of the two subsets of the\n",
    "dataset in parallel. Once the weak learners are trained, they are used to trained a meta learner to combine their predictions and carry out final prediction using the other subset. In test phase, each model predicts its label, these set of labels are fed to the meta learner which generates the final prediction.\n",
    "\n",
    "<img src=\"Images/stack.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a962d-4f4a-473b-9564-2406b72f5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "models = [ ('lr', LinearRegression()),('dt', DecisionTreeRegressor()]\n",
    "stacking = StackingRegressor(estimators=models, final_estimator=RandomForestRegressor(n_estimators=10,random_state=42)\n",
    "stacking.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46214a7a-679c-41f4-9c8e-5ab57e0ba0b3",
   "metadata": {},
   "source": [
    "<img src=\"Images/bbs.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89293178-1b48-4846-94d7-d63d4e08272d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27db9eac-46e1-4154-883a-4a55cf2852ea",
   "metadata": {},
   "source": [
    "## PART 2 <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178d242-2425-416f-b3c5-c310dfd6de75",
   "metadata": {},
   "source": [
    "### Cross Validation <a class=\"anchor\" id=\"section_4_1\"></a>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a81e54-1828-4b9a-83b6-2261c6de6ea7",
   "metadata": {},
   "source": [
    "**Cross-validation** is a statistical method used to estimate the performance (or accuracy) of machine learning models. It is used to protect against overfitting in a predictive model, particularly in a case where the amount of data may be limited. In cross-validation, you make a fixed number of folds (or partitions) of the data, run the analysis on each fold, and then average the overall error estimate.\n",
    "\n",
    "<font color='Brown'>**Need of CV:**</font>\n",
    "\n",
    "1. To Avoid Overfitting:\n",
    "When we train a model on the training set, it tends to overfit most of the time, thus we utilise regularisation approaches to avoid this. Because we only have a few training instances, we must be cautious while lowering the number of training samples and conserving them for testing.\n",
    "\n",
    "2. Support Model tuning:\n",
    "Finding the best combination of model parameters is a common step to tune an algorithm toward learning the dataset’s hidden patterns. But doing this step on a simple training-testing split is typically not recommended. The model performance is usually very sensitive to such parameters and adjusting those based on a predefined dataset split should be avoided. It can cause the model to overfit and reduce its ability to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb5f1d-7461-487c-bc2e-5ce6f5a35ac2",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Types of CV:**</font>\n",
    "\n",
    "1. Train/Test Split: Taken to one extreme, k may be set to 2 (not 1) such that a single train/test split is created to evaluate the model.\n",
    "2. K-Fold Cross Validation\n",
    "3. Stratified K-fold Cross-Validation\n",
    "4. Leave One-out Cross Validation\n",
    "5. Holdout Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b660c48-e0f4-49b9-af48-eb694f3c1771",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "<font color='Brown'>**How it works:**</font>\n",
    "\n",
    "\n",
    "1. Pick a number of folds – K. Usually, k is 5 or 10 but you can choose any number which is less than the dataset’s length.\n",
    "2. Split the dataset into k equal (if possible) parts (they are called folds)\n",
    "3. Choose k – 1 folds as the training set. The remaining fold will be the test set\n",
    "4. Train the model on the training set. On each iteration of cross-validation, you must train a new model independently of the model trained on the previous iteration\n",
    "5. Validate on the test set and save the result\n",
    "6. Repeat steps 3 – 6 *K* times. Each time use the remaining  fold as the test set. In the end, you should have validated the model on every fold that you have.\n",
    "\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/16042grid_search_cross_validation.png\" width=\"500\"/> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec6a2e-9c1c-4005-b95b-10c143a5512c",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Advantages of Cross-Validation :**</font>\n",
    "\n",
    "1. Use All Your Data\n",
    "2. Parameters Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a88b0f4-d13a-460d-ba5c-629967747537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# print(cross_val_score(model, X_train, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aaee2f-5d75-47ee-bc58-b8f850ac5877",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning <a class=\"anchor\" id=\"section_5_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a846ed-f6f1-4221-b411-273f19cca404",
   "metadata": {},
   "source": [
    "Choosing the correct set of hyperparameters to tune the models minimizes the loss function and achieves better results. \n",
    "\n",
    "**Model parameters:** These are the parameters that are estimated by the model from the given data. <br>\n",
    "**Model hyperparameters:** These are the parameters that cannot be estimated by the model from the given data. These parameters are used to estimate the model parameters.\n",
    "\n",
    "<font color='Brown'>**How it works:**</font>\n",
    "\n",
    "Cross-Validation has two main steps: splitting the data into subsets (called folds) and rotating the training and validation among them. The splitting technique commonly has the following properties:\n",
    "\n",
    "- Each fold has approximately the same size.\n",
    "- Data can be randomly selected in each fold or stratified.​\n",
    "- All folds are used to train the model except one, which is used for validation. That validation fold should be rotated until all folds have become a validation fold once and only once.​\n",
    "- Each example is recommended to be contained in one and only one fold.​\n",
    "\n",
    "K-fold and CV are two terms that are used interchangeably. K-fold is just describing how many folds you want to split your dataset into. Many libraries use k=10 as a default value representing 90% going to training and 10% going to the validation set. The next figure describes the process of iterating over the picked ten folds of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb0997-b3a9-448c-bc74-3613d38c34b4",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Types of Hyperparameter tuning:**</font>  \n",
    "\n",
    "1. **Manual:** select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results.\n",
    "2. **Grid Search:** set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient!\n",
    "3. **Random search:** set up a grid of hyperparameter values and select random combinations to train the model and score. The number of search iterations is set based on time/resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9c3df-f038-4e20-9ab0-cc5c374cccb2",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Important Parameters:**</font>  \n",
    "\n",
    "- **get_params** -->  Get parameters for this estimator.\n",
    "\n",
    "- **cv** -->  Determines the cross-validation splitting strategy - *None*, to use the default 5-fold cross validation,\n",
    "\n",
    "- **best_estimator_** -->  Estimator which gave highest score (or smallest loss if specified) on the left out data\n",
    "\n",
    "- **best_score_** -->  Mean cross-validated score of the best_estimator. \n",
    "\n",
    "- **best_params_** -->  Parameter setting that gave the best results on the hold out data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136c27a-ac54-4add-8a3b-a1b2d1b3032b",
   "metadata": {},
   "source": [
    "#### 1. GridSearchCV <a class=\"anchor\" id=\"section_5_1_1\"></a>\n",
    "\n",
    "\n",
    "In the grid search method, we create a grid of possible values for hyperparameters. Each iteration tries a combination of hyperparameters in a specific order. It fits the model on each combination of hyperparameters possible and records the model performance. Finally, it returns the best model with the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b70f6d-59f2-48da-ae24-9b4a6226815a",
   "metadata": {},
   "source": [
    "- **param_grid** -->  Dictionary with parameters names (str) as keys and distributions or lists of parameters to try. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25c0db-8102-4b6b-9ce2-80a1a46e80be",
   "metadata": {},
   "source": [
    "#### 2. RandomsearchCV <a class=\"anchor\" id=\"section_5_1_2\"></a>\n",
    "\n",
    "In the random search method, we create a grid of possible values for hyperparameters. Each iteration tries a random combination of hyperparameters from this grid, records the performance, and lastly returns the combination of hyperparameters that provided the best performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31524394-b3b7-4cdb-9921-6c5c6a2593e5",
   "metadata": {},
   "source": [
    "- **param_distributions** -->  Dictionary with parameters names (str) as keys and distributions or lists of parameters to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36c5f0-67c0-4df6-b66b-5e795de07e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d8c8764-bf40-4a55-aad1-f6ca16409786",
   "metadata": {},
   "source": [
    "### Model Comparison <a class=\"anchor\" id=\"section_6_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7aa4d2-7ffa-4879-b104-ef1f8f3b8915",
   "metadata": {},
   "source": [
    "1. Time complexity\n",
    "\n",
    "2. Space complexity\n",
    "\n",
    "3. Sample complexity\n",
    "\n",
    "4. Bias-variance tradeoff\n",
    "\n",
    "5. Methodology, Assumptions and Objectives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
