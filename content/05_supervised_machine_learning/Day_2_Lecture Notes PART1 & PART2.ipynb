{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d6d8c6-e542-4ebe-954c-024c85d116f0",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning Models - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c4bf7-2dca-471a-a867-ac36da82fa77",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [PART 1](#chapter1)  \n",
    "\n",
    "    * [Ensemble Learning algorithms](#section_1_1)\n",
    "        1. [Bagging algorithms](#Section_1_1_1)\n",
    "        2. [Random Forest](#section_2_1_1)\n",
    "        3. [Boosting algorithms](#section_3_1_1)\n",
    "             * [Gradient Boosting](#section_3_2_1)\n",
    "             * [XGBoost & AdaBoost](#section_3_2_2)\n",
    "        4. [Stacking](#section_4_1_1)\n",
    "<br>\n",
    "\n",
    "* [PART 2](#chapter2)\n",
    "\n",
    "    * [Cross Validation](#section_4_1)\n",
    "    * [HyperParameter Tuning](#section_5_1)\n",
    "        * [GridsearchCV](#section_5_1_1)\n",
    "        * [RandomSearchCV](#section_5_1_2)\n",
    "    * [Model Comparison](#section_6_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b835597-ecfc-4f80-8cf8-6636083414de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImportant Notebook tips\\n\\n<font color=\\'Brown\\'>**Binary Classifier:**</font>\\n\\n<br>\\n\\n<img src=\"https://editor.analyticsvidhya.com/uploads/85598tomek.png\" width=\"300\"/> <br>\\n\\n<p float=\"left\">\\n<img src=\"Images/ML_3.png\" width=\"300\"/>\\n<img src=\"Images/ML2.png\" width=\"450\"/>\\n<p>\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Important Notebook tips\n",
    "\n",
    "<font color='Brown'>**Binary Classifier:**</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/85598tomek.png\" width=\"300\"/> <br>\n",
    "\n",
    "<p float=\"left\">\n",
    "<img src=\"Images/ML_3.png\" width=\"300\"/>\n",
    "<img src=\"Images/ML2.png\" width=\"450\"/>\n",
    "<p>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8af364-9270-4ba7-8215-825c0717ef19",
   "metadata": {},
   "source": [
    "## PART 1 <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fcb1dc-50f9-4f24-b1bc-d842f57d0c8b",
   "metadata": {},
   "source": [
    "## Ensemble Learning algorithms <a class=\"anchor\" id=\"section_1_1\"></a>\n",
    "\n",
    "Ensemble learning models are simply combinations of different machine learning models.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b91607-0f95-4aa7-88db-ab953213ebfd",
   "metadata": {},
   "source": [
    "### 1. Bagging algorithms <a class=\"anchor\" id=\"Section_1_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1eb27f-2934-4c6b-8c05-0c96f946df4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf87c9a8-87ec-4fce-be22-3bb85c871cc4",
   "metadata": {},
   "source": [
    "### 2. Random Forest <a class=\"anchor\" id=\"section_2_1_1\"> </a>\n",
    "- Random forests” is an ensemble of the same type of models, decision trees.\n",
    "- Decision trees are a popular method for various machine learning tasks mostly because their interpretability is very high. A decision tree is a series of filters on the predictor variables. The series of filters end up in a class prediction. Each filter is a binary yes/no question, which creates bifurcations in the series of filters thus leading to a treelike structure. The filters are dependent on the type of predictor variables. If the variables are categorical, such as gender, then the filters could be “is gender female” type of questions. If the variables are continuous, such as gene expression, the filter could be “is PIGX expression larger than 210?”. Every point where we filter samples based on these questions are called “decision nodes”. The tree-fitting algorithm finds the best variables at decision nodes depending on how well they split the samples into classes after the application of the decision node. Decision trees handle both categorical and numeric predictor variables, they are easy to interpret, and they can deal with missing variables. Despite their advantages, decision trees tend to overfit if they are grown very deep and can learn irregular patterns. There are many variants of tree-based machine learning algorithms.\n",
    "\n",
    "- Random forests are devised to counter the shortcomings of decision trees. They are simply ensembles of decision trees. Each tree is trained with a different randomly selected part of the data with randomly selected predictor variables. The goal of introducing randomness is to reduce the variance of the model so it does not overfit, at the expense of a small increase in the bias and some loss of interpretability. This strategy generally boosts the performance of the final model.\n",
    "- The random forests algorithm tries to decorrelate the trees so that they learn different things about the data. It does this by selecting **a random subset of variables.** If one or a few predictor variables are very strong predictors for the response variable, these features will be selected in many of the trees, causing them to become correlated. Random subsampling of predictor variables ensures that not always the best predictors overall are selected for every tree and, the model does have a chance to learn other features of the data.\n",
    "- Another sampling method introduced when building random forest models is **bootstrap resampling** before constructing each tree. This brings the advantage of out-of-the-bag (OOB) error prediction. In this case, the prediction error can be estimated for training samples that were OOB, meaning they were not used in the training, for some percentage of the trees. The prediction error for each sample can be estimated from the trees where that sample was OOB. OOB estimates claimed to be a good alternative to cross-validation estimated errors.\n",
    "\n",
    "<img src=\"Images/rf.png\" width=\"1000\"/>\n",
    "\n",
    "**Parameters:**\n",
    "- For random forests, we have two critical arguments. One of the most critical arguments for random forest is the number of predictor variables to sample in each split of the tree. This parameter controls the independence between the trees, and as explained before, this limits overfitting.\n",
    "- Another variable we can tune is the minimum node size of terminal nodes in the trees (min.node.size). This controls the depth of the trees grown. Setting this to larger numbers might cost a small loss in accuracy but the algorithm will run faster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42d15c-3ea8-4686-86b3-1624c26fbd41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "382523b7-955f-4526-b640-4307bf4149be",
   "metadata": {},
   "source": [
    "### 3. Boosting algorithms <a class=\"anchor\" id=\"section_3_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9dfadd-37b0-4543-8f0d-52f47951df72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4467e4f7-5503-4d26-910d-be4405e18ec0",
   "metadata": {},
   "source": [
    "#### 3.A Gradient Boosting <a class=\"anchor\" id=\"section_3_2_1\"></a>\n",
    "- Gradient boosting is a prediction model that uses an ensemble of decision trees similar to random forest. However, the decision trees are added sequentially, which is why these models are also called “Multiple Additive Regression Trees (MART)” (Friedman and Meulman 2003). Apart from this, you will see similar methods called “Gradient boosting machines (GBM)”(J. H. Friedman 2001) or “Boosted regression trees (BRT)” (Elith, Leathwick, and Hastie 2008) in the literature.\n",
    "\n",
    "- Generally, “boosting” refers to an iterative learning approach where each new model tries to focus on data points where the previous ensemble of simple models did not predict well. Gradient boosting is an improvement over that, where each new model tries to focus on the residual errors (prediction error for the current ensemble of models) of the previous model. Specifically in gradient boosting, the simple models are trees. As in random forests, many trees are grown but in this case, trees are sequentially grown and each tree focuses on fixing the shortcomings of the previous trees. \n",
    "\n",
    "<img src=\"Images/gb.png\" width=\"1000\"/>\n",
    "\n",
    "- One of the most widely used algorithms for gradient boosting is XGboost which stands for “extreme gradient boosting” (Chen and Guestrin 2016). Below we will demonstrate how to use this on our problem. XGboost as well as other gradient boosting methods has many parameters to regularize and optimize the complexity of the model. Finding the best parameters for your problem might take some time. However, this flexibility comes with benefits; methods depending on XGboost have won many machine learning competitions.\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "- The most important parameters are number of trees (nrounds), tree depth (max_depth), and learning rate or shrinkage (eta). Generally, the more trees we have, the better the algorithm will learn because each tree tries to fix classification errors that the previous tree ensemble could not perform. Having too many trees might cause overfitting. However, the learning rate parameter, eta, combats that by shrinking the contribution of each new tree. This can be set to lower values if you have many trees. You can either set a large number of trees and then tune the model with the learning rate parameter or set the learning rate low, say to  0.01  or  0.1  and tune the number of trees. Similarly, tree depth also controls for overfitting. The deeper the tree, the more usually it will overfit. This has to be tuned as well; the default is at 6. You can try to explore a range around the default. Apart from these, as in random forests, you can subsample the training data and/or the predictive variables. These strategies can also help you counter overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d276241-d681-450d-8199-5b0826595011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9463bf9f-c3d3-4313-8f33-9cd7774e0e0e",
   "metadata": {},
   "source": [
    "#### 3.B XG Boosting & ADA Boosting <a class=\"anchor\" id=\"section_3_2_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b15a4-8a6a-41c3-ad3b-0eec8a8b38a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be9f8231-18be-4d82-b911-f16a9a8f7672",
   "metadata": {},
   "source": [
    "### 4 Stacking <a class=\"anchor\" id=\"section_4_4_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a962d-4f4a-473b-9564-2406b72f5403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c549b72d-2637-4242-bc21-7724b7eb8a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89293178-1b48-4846-94d7-d63d4e08272d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27db9eac-46e1-4154-883a-4a55cf2852ea",
   "metadata": {},
   "source": [
    "## PART 2 <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178d242-2425-416f-b3c5-c310dfd6de75",
   "metadata": {},
   "source": [
    "### Cross Validation <a class=\"anchor\" id=\"section_4_1\"></a>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a81e54-1828-4b9a-83b6-2261c6de6ea7",
   "metadata": {},
   "source": [
    "**Cross-validation** is a statistical method used to estimate the performance (or accuracy) of machine learning models. It is used to protect against overfitting in a predictive model, particularly in a case where the amount of data may be limited. In cross-validation, you make a fixed number of folds (or partitions) of the data, run the analysis on each fold, and then average the overall error estimate.\n",
    "\n",
    "<font color='Brown'>**Need of CV:**</font>\n",
    "\n",
    "1. To Avoid Overfitting:\n",
    "When we train a model on the training set, it tends to overfit most of the time, thus we utilise regularisation approaches to avoid this. Because we only have a few training instances, we must be cautious while lowering the number of training samples and conserving them for testing.\n",
    "\n",
    "2. Support Model tuning:\n",
    "Finding the best combination of model parameters is a common step to tune an algorithm toward learning the dataset’s hidden patterns. But doing this step on a simple training-testing split is typically not recommended. The model performance is usually very sensitive to such parameters and adjusting those based on a predefined dataset split should be avoided. It can cause the model to overfit and reduce its ability to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb5f1d-7461-487c-bc2e-5ce6f5a35ac2",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Types of CV:**</font>\n",
    "\n",
    "1. Train/Test Split: Taken to one extreme, k may be set to 2 (not 1) such that a single train/test split is created to evaluate the model.\n",
    "2. K-Fold Cross Validation\n",
    "3. Stratified K-fold Cross-Validation\n",
    "4. Leave One-out Cross Validation\n",
    "5. Holdout Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b660c48-e0f4-49b9-af48-eb694f3c1771",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "<font color='Brown'>**How it works:**</font>\n",
    "\n",
    "\n",
    "1. Pick a number of folds – K. Usually, k is 5 or 10 but you can choose any number which is less than the dataset’s length.\n",
    "2. Split the dataset into k equal (if possible) parts (they are called folds)\n",
    "3. Choose k – 1 folds as the training set. The remaining fold will be the test set\n",
    "4. Train the model on the training set. On each iteration of cross-validation, you must train a new model independently of the model trained on the previous iteration\n",
    "5. Validate on the test set and save the result\n",
    "6. Repeat steps 3 – 6 *K* times. Each time use the remaining  fold as the test set. In the end, you should have validated the model on every fold that you have.\n",
    "\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/16042grid_search_cross_validation.png\" width=\"500\"/> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec6a2e-9c1c-4005-b95b-10c143a5512c",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Advantages of Cross-Validation :**</font>\n",
    "\n",
    "1. Use All Your Data\n",
    "2. Parameters Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a88b0f4-d13a-460d-ba5c-629967747537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# print(cross_val_score(model, X_train, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aaee2f-5d75-47ee-bc58-b8f850ac5877",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning <a class=\"anchor\" id=\"section_5_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a846ed-f6f1-4221-b411-273f19cca404",
   "metadata": {},
   "source": [
    "Choosing the correct set of hyperparameters to tune the models minimizes the loss function and achieves better results. \n",
    "\n",
    "**Model parameters:** These are the parameters that are estimated by the model from the given data. <br>\n",
    "**Model hyperparameters:** These are the parameters that cannot be estimated by the model from the given data. These parameters are used to estimate the model parameters.\n",
    "\n",
    "<font color='Brown'>**How it works:**</font>\n",
    "\n",
    "Cross-Validation has two main steps: splitting the data into subsets (called folds) and rotating the training and validation among them. The splitting technique commonly has the following properties:\n",
    "\n",
    "- Each fold has approximately the same size.\n",
    "- Data can be randomly selected in each fold or stratified.​\n",
    "- All folds are used to train the model except one, which is used for validation. That validation fold should be rotated until all folds have become a validation fold once and only once.​\n",
    "- Each example is recommended to be contained in one and only one fold.​\n",
    "\n",
    "K-fold and CV are two terms that are used interchangeably. K-fold is just describing how many folds you want to split your dataset into. Many libraries use k=10 as a default value representing 90% going to training and 10% going to the validation set. The next figure describes the process of iterating over the picked ten folds of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb0997-b3a9-448c-bc74-3613d38c34b4",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Types of Hyperparameter tuning:**</font>  \n",
    "\n",
    "1. **Manual:** select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results.\n",
    "2. **Grid Search:** set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient!\n",
    "3. **Random search:** set up a grid of hyperparameter values and select random combinations to train the model and score. The number of search iterations is set based on time/resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9c3df-f038-4e20-9ab0-cc5c374cccb2",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Important Parameters:**</font>  \n",
    "\n",
    "- **get_params** -->  Get parameters for this estimator.\n",
    "\n",
    "- **cv** -->  Determines the cross-validation splitting strategy - *None*, to use the default 5-fold cross validation,\n",
    "\n",
    "- **best_estimator_** -->  Estimator which gave highest score (or smallest loss if specified) on the left out data\n",
    "\n",
    "- **best_score_** -->  Mean cross-validated score of the best_estimator. \n",
    "\n",
    "- **best_params_** -->  Parameter setting that gave the best results on the hold out data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136c27a-ac54-4add-8a3b-a1b2d1b3032b",
   "metadata": {},
   "source": [
    "#### 1. GridSearchCV <a class=\"anchor\" id=\"section_5_1_1\"></a>\n",
    "\n",
    "\n",
    "In the grid search method, we create a grid of possible values for hyperparameters. Each iteration tries a combination of hyperparameters in a specific order. It fits the model on each combination of hyperparameters possible and records the model performance. Finally, it returns the best model with the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b70f6d-59f2-48da-ae24-9b4a6226815a",
   "metadata": {},
   "source": [
    "- **param_grid** -->  Dictionary with parameters names (str) as keys and distributions or lists of parameters to try. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25c0db-8102-4b6b-9ce2-80a1a46e80be",
   "metadata": {},
   "source": [
    "#### 2. RandomsearchCV <a class=\"anchor\" id=\"section_5_1_2\"></a>\n",
    "\n",
    "In the random search method, we create a grid of possible values for hyperparameters. Each iteration tries a random combination of hyperparameters from this grid, records the performance, and lastly returns the combination of hyperparameters that provided the best performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31524394-b3b7-4cdb-9921-6c5c6a2593e5",
   "metadata": {},
   "source": [
    "- **param_distributions** -->  Dictionary with parameters names (str) as keys and distributions or lists of parameters to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36c5f0-67c0-4df6-b66b-5e795de07e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d8c8764-bf40-4a55-aad1-f6ca16409786",
   "metadata": {},
   "source": [
    "### Model Comparison <a class=\"anchor\" id=\"section_6_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7aa4d2-7ffa-4879-b104-ef1f8f3b8915",
   "metadata": {},
   "source": [
    "1. Time complexity\n",
    "\n",
    "2. Space complexity\n",
    "\n",
    "3. Sample complexity\n",
    "\n",
    "4. Bias-variance tradeoff\n",
    "\n",
    "5. Methodology, Assumptions and Objectives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
