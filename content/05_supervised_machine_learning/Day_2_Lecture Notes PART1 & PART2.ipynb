{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d6d8c6-e542-4ebe-954c-024c85d116f0",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning Models - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c4bf7-2dca-471a-a867-ac36da82fa77",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [PART 1](#chapter1)  \n",
    "\n",
    "    * [Ensemble Learning algorithms](#section_1_1)\n",
    "        1. [Bagging algorithms](#Section_1_1_1)\n",
    "        2. [Random Forest](#section_2_1_1)\n",
    "        3. [Boosting algorithms](#section_3_1_1)\n",
    "             * [Gradient Boosting](#section_3_2_1)\n",
    "             * [XGBoost & AdaBoost](#section_3_2_2)\n",
    "        4. [Stacking](#section_4_1_1)\n",
    "<br>\n",
    "\n",
    "* [PART 2](#chapter2)\n",
    "\n",
    "    * [Cross Validation](#section_4_1)\n",
    "    * [HyperParameter Tuning](#section_5_1)\n",
    "        * [GridsearchCV](#section_5_1_1)\n",
    "        * [RandomSearchCV](#section_5_1_2)\n",
    "    * [Model Comparison](#section_6_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b835597-ecfc-4f80-8cf8-6636083414de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImportant Notebook tips\\n\\n<font color=\\'Brown\\'>**Binary Classifier:**</font>\\n\\n<br>\\n\\n<img src=\"https://editor.analyticsvidhya.com/uploads/85598tomek.png\" width=\"300\"/> <br>\\n\\n<p float=\"left\">\\n<img src=\"Images/ML_3.png\" width=\"300\"/>\\n<img src=\"Images/ML2.png\" width=\"450\"/>\\n<p>\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Important Notebook tips\n",
    "\n",
    "<font color='Brown'>**Binary Classifier:**</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/85598tomek.png\" width=\"300\"/> <br>\n",
    "\n",
    "<p float=\"left\">\n",
    "<img src=\"Images/ML_3.png\" width=\"300\"/>\n",
    "<img src=\"Images/ML2.png\" width=\"450\"/>\n",
    "<p>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8af364-9270-4ba7-8215-825c0717ef19",
   "metadata": {},
   "source": [
    "## PART 1 <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fcb1dc-50f9-4f24-b1bc-d842f57d0c8b",
   "metadata": {},
   "source": [
    "## Ensemble Learning algorithms <a class=\"anchor\" id=\"section_1_1\"></a>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b91607-0f95-4aa7-88db-ab953213ebfd",
   "metadata": {},
   "source": [
    "### 1. Bagging algorithms <a class=\"anchor\" id=\"Section_1_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1eb27f-2934-4c6b-8c05-0c96f946df4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf87c9a8-87ec-4fce-be22-3bb85c871cc4",
   "metadata": {},
   "source": [
    "### 2. Random Forest <a class=\"anchor\" id=\"section_2_1_1\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42d15c-3ea8-4686-86b3-1624c26fbd41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "382523b7-955f-4526-b640-4307bf4149be",
   "metadata": {},
   "source": [
    "### 3. Boosting algorithms <a class=\"anchor\" id=\"section_3_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9dfadd-37b0-4543-8f0d-52f47951df72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4467e4f7-5503-4d26-910d-be4405e18ec0",
   "metadata": {},
   "source": [
    "#### 3.A Gradient Boosting <a class=\"anchor\" id=\"section_3_2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d276241-d681-450d-8199-5b0826595011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9463bf9f-c3d3-4313-8f33-9cd7774e0e0e",
   "metadata": {},
   "source": [
    "#### 3.B XG Boosting & ADA Boosting <a class=\"anchor\" id=\"section_3_2_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b15a4-8a6a-41c3-ad3b-0eec8a8b38a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be9f8231-18be-4d82-b911-f16a9a8f7672",
   "metadata": {},
   "source": [
    "### 4 Stacking <a class=\"anchor\" id=\"section_4_4_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a962d-4f4a-473b-9564-2406b72f5403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c549b72d-2637-4242-bc21-7724b7eb8a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89293178-1b48-4846-94d7-d63d4e08272d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27db9eac-46e1-4154-883a-4a55cf2852ea",
   "metadata": {},
   "source": [
    "## PART 2 <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178d242-2425-416f-b3c5-c310dfd6de75",
   "metadata": {},
   "source": [
    "### Cross Validation <a class=\"anchor\" id=\"section_4_1\"></a>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a81e54-1828-4b9a-83b6-2261c6de6ea7",
   "metadata": {},
   "source": [
    "**Cross-validation** is a statistical method used to estimate the performance (or accuracy) of machine learning models. It is used to protect against overfitting in a predictive model, particularly in a case where the amount of data may be limited. In cross-validation, you make a fixed number of folds (or partitions) of the data, run the analysis on each fold, and then average the overall error estimate.\n",
    "\n",
    "<font color='Brown'>**Need of CV:**</font>\n",
    "\n",
    "1. To Avoid Overfitting:\n",
    "When we train a model on the training set, it tends to overfit most of the time, thus we utilise regularisation approaches to avoid this. Because we only have a few training instances, we must be cautious while lowering the number of training samples and conserving them for testing.\n",
    "\n",
    "2. Support Model tuning:\n",
    "Finding the best combination of model parameters is a common step to tune an algorithm toward learning the dataset’s hidden patterns. But doing this step on a simple training-testing split is typically not recommended. The model performance is usually very sensitive to such parameters and adjusting those based on a predefined dataset split should be avoided. It can cause the model to overfit and reduce its ability to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb5f1d-7461-487c-bc2e-5ce6f5a35ac2",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Types of CV:**</font>\n",
    "\n",
    "1. Train/Test Split: Taken to one extreme, k may be set to 2 (not 1) such that a single train/test split is created to evaluate the model.\n",
    "2. K-Fold Cross Validation\n",
    "3. Stratified K-fold Cross-Validation\n",
    "4. Leave One-out Cross Validation\n",
    "5. Holdout Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b660c48-e0f4-49b9-af48-eb694f3c1771",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "<font color='Brown'>**How it works:**</font>\n",
    "\n",
    "\n",
    "1. Pick a number of folds – K. Usually, k is 5 or 10 but you can choose any number which is less than the dataset’s length.\n",
    "2. Split the dataset into k equal (if possible) parts (they are called folds)\n",
    "3. Choose k – 1 folds as the training set. The remaining fold will be the test set\n",
    "4. Train the model on the training set. On each iteration of cross-validation, you must train a new model independently of the model trained on the previous iteration\n",
    "5. Validate on the test set and save the result\n",
    "6. Repeat steps 3 – 6 *K* times. Each time use the remaining  fold as the test set. In the end, you should have validated the model on every fold that you have.\n",
    "\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/16042grid_search_cross_validation.png\" width=\"500\"/> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec6a2e-9c1c-4005-b95b-10c143a5512c",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Advantages of Cross-Validation :**</font>\n",
    "\n",
    "1. Use All Your Data\n",
    "2. Parameters Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a88b0f4-d13a-460d-ba5c-629967747537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# print(cross_val_score(model, X_train, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aaee2f-5d75-47ee-bc58-b8f850ac5877",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning <a class=\"anchor\" id=\"section_5_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a846ed-f6f1-4221-b411-273f19cca404",
   "metadata": {},
   "source": [
    "Choosing the correct set of hyperparameters to tune the models minimizes the loss function and achieves better results. \n",
    "\n",
    "**Model parameters:** These are the parameters that are estimated by the model from the given data. <br>\n",
    "**Model hyperparameters:** These are the parameters that cannot be estimated by the model from the given data. These parameters are used to estimate the model parameters.\n",
    "\n",
    "<font color='Brown'>**How it works:**</font>\n",
    "\n",
    "Cross-Validation has two main steps: splitting the data into subsets (called folds) and rotating the training and validation among them. The splitting technique commonly has the following properties:\n",
    "\n",
    "- Each fold has approximately the same size.\n",
    "- Data can be randomly selected in each fold or stratified.​\n",
    "- All folds are used to train the model except one, which is used for validation. That validation fold should be rotated until all folds have become a validation fold once and only once.​\n",
    "- Each example is recommended to be contained in one and only one fold.​\n",
    "\n",
    "K-fold and CV are two terms that are used interchangeably. K-fold is just describing how many folds you want to split your dataset into. Many libraries use k=10 as a default value representing 90% going to training and 10% going to the validation set. The next figure describes the process of iterating over the picked ten folds of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb0997-b3a9-448c-bc74-3613d38c34b4",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Types of Hyperparameter tuning:**</font>  \n",
    "\n",
    "1. **Manual:** select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results.\n",
    "2. **Grid Search:** set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient!\n",
    "3. **Random search:** set up a grid of hyperparameter values and select random combinations to train the model and score. The number of search iterations is set based on time/resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9c3df-f038-4e20-9ab0-cc5c374cccb2",
   "metadata": {},
   "source": [
    "<font color='Brown'>**Important Parameters:**</font>  \n",
    "\n",
    "- **get_params** -->  Get parameters for this estimator.\n",
    "\n",
    "- **cv** -->  Determines the cross-validation splitting strategy - *None*, to use the default 5-fold cross validation,\n",
    "\n",
    "- **best_estimator_** -->  Estimator which gave highest score (or smallest loss if specified) on the left out data\n",
    "\n",
    "- **best_score_** -->  Mean cross-validated score of the best_estimator. \n",
    "\n",
    "- **best_params_** -->  Parameter setting that gave the best results on the hold out data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136c27a-ac54-4add-8a3b-a1b2d1b3032b",
   "metadata": {},
   "source": [
    "#### 1. GridSearchCV <a class=\"anchor\" id=\"section_5_1_1\"></a>\n",
    "\n",
    "\n",
    "In the grid search method, we create a grid of possible values for hyperparameters. Each iteration tries a combination of hyperparameters in a specific order. It fits the model on each combination of hyperparameters possible and records the model performance. Finally, it returns the best model with the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b70f6d-59f2-48da-ae24-9b4a6226815a",
   "metadata": {},
   "source": [
    "- **param_grid** -->  Dictionary with parameters names (str) as keys and distributions or lists of parameters to try. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25c0db-8102-4b6b-9ce2-80a1a46e80be",
   "metadata": {},
   "source": [
    "#### 2. RandomsearchCV <a class=\"anchor\" id=\"section_5_1_2\"></a>\n",
    "\n",
    "In the random search method, we create a grid of possible values for hyperparameters. Each iteration tries a random combination of hyperparameters from this grid, records the performance, and lastly returns the combination of hyperparameters that provided the best performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31524394-b3b7-4cdb-9921-6c5c6a2593e5",
   "metadata": {},
   "source": [
    "- **param_distributions** -->  Dictionary with parameters names (str) as keys and distributions or lists of parameters to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36c5f0-67c0-4df6-b66b-5e795de07e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d8c8764-bf40-4a55-aad1-f6ca16409786",
   "metadata": {},
   "source": [
    "### Model Comparison <a class=\"anchor\" id=\"section_6_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7aa4d2-7ffa-4879-b104-ef1f8f3b8915",
   "metadata": {},
   "source": [
    "1. Time complexity\n",
    "\n",
    "2. Space complexity\n",
    "\n",
    "3. Sample complexity\n",
    "\n",
    "4. Bias-variance tradeoff\n",
    "\n",
    "5. Methodology, Assumptions and Objectives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
