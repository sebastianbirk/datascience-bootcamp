{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## <ins>Data Science Bootcamp - 3th Day- Statistics and ML</ins>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this chapter, we will take a look at the fundamentals of statistics needed for machine learning (from now on ML) and get an overview of the different kinds of machine learning. Due to the scope of topics, this will be a general overview. Later on in the bootcamp, there will be a more detailed look into the machine learning topics.<br>\n",
    "Most topics will be be accompanied by exertices on the topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Table of content:\n",
    "\n",
    "* [Data types](#ten)\n",
    "* [Probability theory](#one)\n",
    "* [Causality and correlation](#two)\n",
    "* [Moments](#three)\n",
    "* [Distributions](#four)\n",
    "* [Confidence intervals](#five)\n",
    "* [Hypothesis testing and signifinace tests](#six)\n",
    "* [Mathematical basics for ML](#seven)\n",
    "* [Types of ML](#eight)\n",
    "* [Deep learning](#nine)\n",
    "* [Learning](#ten)\n",
    "\n",
    "\n",
    "________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "## Data types \n",
    "Data is information. But not all information is the same, data can fall into several different categories. <br>\n",
    "There are two types of data: Qualitative and Quantitative data, which are further classified into four types of data: nominal, ordinal, discrete, and continuous.<br>\n",
    "- nominal data is characterized by no clear order between the categories, has no order, nothing in between them<br>\n",
    "- ordinal data is characterized by a clear distinction between the categories and have a order, but nothing in between them\n",
    "- discrete data is characterized by a clear distinction between the categories, moments like mean can be calulated.\n",
    "- continous data characterized by a continuous (even values that are arbitrarily close together) distribution.\n",
    "\n",
    "\n",
    "<img src=\"datatypes.jpg\" style=\"width: 350px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br><br>\n",
    "## Time series: \n",
    "Time series are a series of data, that is indexed by a time scale.<br>Compared to non time dependent data, there are other phemomena and  have to be taken into account. Also different models should be used.<br>\n",
    "\n",
    "- Autocorrelation\n",
    "- Seasonality\n",
    "- etc.\n",
    "<br>\n",
    "<img src=\"time_series.jpg\" style=\"width: 450px;\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Probability theory <a class=\"anchor\" id=\"one\"></a>\n",
    "1. Probabilities are described in numbers between 0 and 1.\n",
    ">  $P(X)=0.1$ describes the probability of an Event X happing to be 10%.<br><br>\n",
    "2. All possible events/probabilities put together have the probability of 1.<br>\n",
    "> The probiability of heads in a coin toss is 0.5, and for tails 0.5. <br>They are the only possible events and add up to 1.\n",
    "\n",
    "\n",
    "[Kolmogorov axioms](https://en.wikipedia.org/wiki/Probability_axioms)  \n",
    "___________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability density function\n",
    "Probabilities are displayed as a probability density function (PDF).<br>\n",
    "PDFs describe the distribution of all probabilities across the possible events.<br>\n",
    "<br> In this example, we have a [continous](https://en.wikipedia.org/wiki/Continuous_function) PDF.\n",
    "We can see, that function has it maximum at 0 with a probability of 0.4.\n",
    "\n",
    "<img src=\"pdf.png\" style=\"width: 300px;\" align=\"left\"/>\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "_________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative distribution function\n",
    "Closely releated to the PDF is the cumulative distribution function (CDF).<br>\n",
    "\n",
    "Mathematically the CDF is the derivative of the PDF.<br>\n",
    "<img src=\"cdf.png\" style=\"width: 300px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "___________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Law of large numbers\n",
    "\n",
    "The law of large numbers is one of the most central laws in statistics.<br>\n",
    "It states, that in a series of random experiments the measured probability will over time converge on the expected probability.<br>\n",
    "<img src=\"Law_large_numbers.png\" style=\"width: 350px;\" align = \"left\"/>\n",
    "<br><br><br><br><br><br><br><br><br><br><br>\n",
    "$\\lim_{x\\to\\infty} \\sum_{i=1}^{n}\\frac{X_i}{n}=\\bar{X}$<br>\n",
    "As we will see later on ML, this is one of the reasons, why ML relies extensivly on repetions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) \n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causality and correlation <a class=\"anchor\" id=\"two\"></a>\n",
    "Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate). Itâ€™s a common tool for describing simple relationships without making a statement about cause and effect.<br>\n",
    "The sample correlation coefficient, r, quantifies the strength of the relationship.\n",
    "Correlations are useful because they can indicate a predictive relationship that can be exploited in practice.<br>\n",
    "\n",
    "$corr(X,Y)=\\frac{E((X-\\mu_x)(Y-\\mu_y))}{\\sigma_x\\cdot\\sigma_y}$<br><br>\n",
    "<img src=\"corr.png\" style=\"width: 350px;\" align = \"left\"/><br><br><br><br><br><br><br><br><br>\n",
    "But Correlation $\\neq$ Causation<br>\n",
    "<img src=\"fun_corr.png\" style=\"width: 550px;\" align = \"left\"/><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "<br>Two such experiments or analyses you can use to identify causation with your product are:\n",
    "\n",
    "- Hypothesis testing\n",
    "- A/B/n experiments (factorial experiment)\n",
    "- Academic research\n",
    "\n",
    "[Correlation](https://en.wikipedia.org/wiki/Correlation)\n",
    "__________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "Moments are measure, that describe the shape of a distribution. The four moments describe distributions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1^{th} moment: $ Mean or center of mass <br>$M_k=\\frac{1}{n}\\sum_{i=1}^n X_i^k$<br>\n",
    "Mean is one of the important and most commonly used measures\n",
    "\n",
    "<img src=\"mean.png\" style=\"width: 200px;\" align = \"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2^{th} moment: $ Variance <br>\n",
    "Variance is the expectation of the squared deviation of a random variable from its population mean or sample mean<br>\n",
    "$M_k=\\sum_{i=1}^n p_i \\cdot (x_i-\\mu)^2$<br>\n",
    "<img src=\"mean.png\" style=\"width: 200px;\" align = \"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3^{th} moment: $ Skewness <br>\n",
    "<img src=\"skewness.png\" style=\"width: 300px;\" align = \"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4^{th} moment: $ Kurtosis <br>\n",
    "Kurtosis describes the \"pointyness\" of a distribution.<br>\n",
    "<img src=\"kurtosis.jpg\" style=\"width: 300px;\" align = \"left\"/>\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "[Moments](https://en.wikipedia.org/wiki/Moment_(mathematics))\n",
    "___________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantils\n",
    "Quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way.  <br> \n",
    "There are three groups of predefined quantile with their own name:<br>\n",
    "- Quartiles (four groups)<br> \n",
    "- Deciles (ten groups)<br>\n",
    "- Percentiles (100 groups)<br><br>\n",
    "<img src=\"quantile.png\" style=\"width: 350px;\" align = \"left\"/><br><br><br><br><br><br><br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "Bias is a systemic error in all measurements. This error is in all measurements and thus skews it away for the true value. <br>\n",
    "Variance is the spread of the datapoints around the mean. <br>\n",
    "\n",
    "<img src=\"bias_variance.png\" style=\"width: 300px;\" align=\"left\"/>\n",
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will later on take a look at [Bias-Variance-Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "Let's have a look at the most important distributions! <br>\n",
    "There are two different kinds of distrubtions: Discrete and continous\n",
    "- Continous variable in gerneral means there is no \"jump\" in the function.\n",
    "- Discrete variable have a \"jump\" in the function.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete:\n",
    "Binomial distribution is a common probability distribution that models the probability of obtaining one of two outcomes under a given number of parameters. It summarizes the number of trials when each trial has the same chance of attaining one specific outcome. The value of a binomial is obtained by multiplying the number of independent trials by the successes.\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"binomial.png\" style=\"width: 300px;\" align = \"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform distribution\n",
    "A discrete uniform distribution is a statistical distribution where the probability of outcomes is equally likely and with finite values.<br> A good example of a discrete uniform distribution would be the possible outcomes of rolling a 6-sided die.<br>\n",
    "<img src=\"Uniform_discrete.svg\" style=\"width: 300px;\" align = \"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous:\n",
    "### Uniform distribution\n",
    "$f(x)=\\frac{1}{b-a}$<br>\n",
    "<br><img src=\"uniform.png\" style=\"width: 300px;\" align = \"left\"/><br><br><br><br><br><br><br><br><br><br>\n",
    "[Uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distributionn)\n",
    "____________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "### Normal distribution\n",
    "The normal distribution, sometimes called the Gaussian distribution, is a two-parameter family of curves.<br> A low of processes follow a ND, e.g. height of humans, IQ, blood pressure etc. <br><br>\n",
    "$f(x)=\\frac{1}{\\sigma\\sqrt{2\\cdot \\pi}}\\cdot e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}$<br>\n",
    "<br><img src=\"normal.png\" style=\"width: 300px;\" align = \"left\"/><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "Note: Binomal distribution convergess to normal distribution. Discrete and continous can thus be hard to tell apart.\n",
    "\n",
    "[Normal distribustion](https://en.wikipedia.org/wiki/Normal_distribution)\n",
    "________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "## Confidence intervals <a class=\"anchor\" id=\"five\"></a>\n",
    "Confidence intervalls are intervalls, where a certain percentage of values lie within.<br>\n",
    "For a confidence niveau of 95%, 95% of alle values will be within the CI.<br>\n",
    "Keep in mind, that each distribtution has a own formula for its confidence intervall.<br>\n",
    "<br>Confidence intervall for normal distribustion:<br><br>\n",
    "$[upper, lower] = \\bar{X}\\pm u\\cdot\\frac{\\sigma}{\\sqrt{n}}$<br>\n",
    "<img src=\"confidence-interval.jpg\" style=\"width: 350px;\" align = \"left\"/><br><br><br><br><br><br><br><br><br><br>\n",
    "Generally for more data the CI narrows down.\n",
    "\n",
    "\n",
    "The name for the probability of is [statistical significance.](https://en.wikipedia.org/wiki/Statistical_significance)\n",
    "\n",
    "\n",
    "\n",
    "[Confidence intervall](https://en.wikipedia.org/wiki/Confidence_interval)\n",
    "_____________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing and signifinace tests <a class=\"anchor\" id=\"six\"></a>\n",
    "Tests are done to check is a given obervation fits to a distribution e.g. normal distrubtion.<br>\n",
    "A test statistic is selected or defined in such a way as to quantify, within observed data, behaviours that would distinguish the null from the alternative hypothesis, where such an alternative is prescribed, or that would characterize the null hypothesis if there is no explicitly stated alternative hypothesis.\n",
    "\n",
    "Null hypothesis $H_0$ vs. alternative hyptohesis $H_1$\n",
    "\n",
    "One sided vs. two sided test:\n",
    "\n",
    "- two sided tests are for checking if a value is equal or different from a expected value.\n",
    "- one sided test are for checking if a value is different in only one direction.\n",
    "\n",
    "<img src=\"one_vs_two.jpg\" style=\"width: 400px;\" align = \"left\"/><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "$H_0: \\mu = \\mu_0$ vs. $H_1: \\mu \\neq \\mu_0$\n",
    "\n",
    "\n",
    "### Normal distribution with two sides\n",
    "\n",
    "\n",
    "We are now using the t-Test <br>\n",
    "Reject $H_0 \\leftrightarrow T=\\sqrt{n}\\cdot\\frac{\\bar{X}-\\mu_0}{\\sigma}>t_{n-1,1-\\alpha} $<br>\n",
    "If this equition is meet the null hypothesis can be rejected and the mean of the obervation is different from expected mean.\n",
    "\n",
    "[Hypothesis testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing)\n",
    "______________________________\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <ins> Maschine Learning Fundamentals</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML definition\n",
    "> ML: The field of study that gives computers the ability to learn without being explicitly programmed. Arthur Samuel (1959).\n",
    "<br>\n",
    "\n",
    "ML-methods are algorithems, that take data and an algorithem and produces an function, that can be used to make new predictions.\n",
    "As such these methods are different from purely mathematical models, where a fixed formula is given and <br><br> \n",
    "\n",
    "<img src=\"ai_ml_dl.png\" style=\"width: 300px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml_def.png\" style=\"width: 300px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied ML\n",
    "Python is right now the most used language in data science and ML. On a survey on Kaggle, 83% of Data Scientists are regularly using Python <br>\n",
    "<img src=\"ml_language.svg\" style=\"width: 500px;\" align=\"left\"/>\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "The most important data science libaries used are:\n",
    "<ul>\n",
    "  <li>Matplotlib</li>\n",
    "  <li>Numpy</li>\n",
    "  <li>Scikit-learn</li>\n",
    "  <li>PyTorch</li>\n",
    "\n",
    "  <li>Keras</li>\n",
    "  <li>Pandas</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three maincategories of ML:\n",
    "- Supervised \n",
    "- Unsupervised \n",
    "- Reiforcment learing<br><br>\n",
    "<img src=\"svl_usvl_rl.png\" style=\"width: 500px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical basics for ML <a class=\"anchor\" id=\"seven\"></a>\n",
    "### Matrix:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical fundamentals of ML are matrixes and arithmetic operations with them. Matrix multiplication is resource intensive and slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\left( \\begin{array}{rrrr}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "\\end{array}\\right) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms:\n",
    "Part of ML is measuring the difference between data points. Two norms are commonly used:\n",
    "Norms are used to measure those differences.<br>\n",
    "L1-Norm:<br>$\\Vert(x)\\Vert_1$<br>\n",
    "<img src=\"L1_norm.jpg\" style=\"width: 200px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br>\n",
    "L2-Norm:<br>$\\Vert(x)\\Vert_2$<br>\n",
    "<img src=\"L2_norm.jpg\" style=\"width: 200px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "Bias-Variance Tradeoff is one of the central problens in ML.<br>\n",
    "The biasâ€“variance dilemma or biasâ€“variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing. <br>\n",
    "<img src=\"overfit_underfit.png\" style=\"width: 450px;\" align=\"left\"/><br><br><br><br><br><br><br>\n",
    "This dilemma leads to the below  tradeoff, we are trying to get to the sweetspot in witch the error of bias and variance is the lowest.<br><br>\n",
    "<img src=\"bias_variance_tradeoff.png\" style=\"width: 300px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br><br><br>\n",
    "In order to check the validity and fitness of the model, data is usually split into 2 categories: <br>\n",
    "- training data on which the model is trained on \n",
    "- test data, on which the model is tested on.<br>\n",
    "A split of 80:20 training/test is usually done.\n",
    "\n",
    "<img src=\"training_test.jpg\" style=\"width: 350px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of ML <a class=\"anchor\" id=\"eight\"></a>\n",
    "### Supervised learning\n",
    "Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.<br>As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately, which occurs as part of the cross validation process.<br><br>\n",
    "<img src=\"supervised.png\" style=\"width: 450px;\" align=\"left\"/>\n",
    "<br><br><br><br><br><br><br><br><br><br><br>\n",
    "Imported supervised algorithms\n",
    "- Nearest Neighbor\n",
    "- Naive Bayes\n",
    "- Decision Trees\n",
    "- Linear Regression\n",
    "- Support Vector Machines (SVM)\n",
    "- Neural Networks*\n",
    "\n",
    "[Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)\n",
    "_______________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Linear regression\n",
    "\n",
    "In order to get a feeling for maschine learning, lets have a look at the most basic form of ML: <br> Linear regression. The bread and butter of data science.<br>It solve a lot of problems, without the use of more advanced models. <br>\n",
    ">$y(x)=\\beta+x\\cdot\\epsilon$\n",
    "\n",
    "Linear regression is a algorthim, that estimates the $\\beta$ and $\\epsilon$ of the above equation for a given dataset.<br><br>\n",
    "- $\\beta$ is called Intercept and is the Y axis-value at which the line intersects with the regression-line.\n",
    "- $\\epsilon$ is the coeffictient, the incline with which the regression line increses.<br>\n",
    "<img src=\"Normdist_regression.png\" style=\"width: 350px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "$R^2$ is the proportion of the variation in the dependent variable that is predictable from the independent variable.<br>\n",
    "Linear regression is the go to option for a lot of statistical analysis. One of the advantages is it has no bias in it. But it has prerequisites:\n",
    "- Linear relationship\n",
    "- No auto-correlation\n",
    "- Homoscedasticity\n",
    "If those conditions are not meat other methods or transformations needed to be taken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss-function\n",
    "Loss-function are a fundamental part of ML. They measure the difference between the predicted value and the expected value. \n",
    "There are different functions, that weight different aspect of the difference:<br>\n",
    "$MSE = \\frac{1}{n}\\cdot \\sum_{i=1}^n (Y_i-Y)^2$.<br>\n",
    "Mean squared error\n",
    "The mean squadared error penalizes values further away from the expected value.<br>\n",
    "There are several different loss functions, each better suited for a different purpose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Linear regression](https://en.wikipedia.org/wiki/Linear_regression)<br>[Loss-function](https://en.wikipedia.org/wiki/Loss_function)\n",
    "__________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning\n",
    "Unsupervied learning is a categorgy of algorithems, that learns from unlabeled data.<br>\n",
    "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).<br>\n",
    "Generlly norms are used to measure the diferences between the data points.\n",
    "\n",
    "<img src=\"kmeans.png\" style=\"width: 300px;\" align=\"left\"/>\n",
    "<br><br><br><br><br><br><br><br>\n",
    "<ul>\n",
    "  <li>Affinity Propagation</li>\n",
    "  <li>Agglomerative Clustering</li>\n",
    "  <li>BIRCH</li>\n",
    "  <li>DBSCANh</li>\n",
    "  <li>K-Means</li>\n",
    "  <li>Mini-Batch K-Means</li>\n",
    "  <li>Mean Shift</li>\n",
    "  <li>OPTICS</li>\n",
    "  <li>Spectral Clustering</li>\n",
    "  <li>Gaussian Mixture Model</li>\n",
    "</ul>\n",
    "\n",
    "Hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:\n",
    "\n",
    "Agglomerative: This is a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.<br>\n",
    "Divisive: This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.<br><br>\n",
    "<img src=\"Hirachical.png\" style=\"width: 300px;\" align=\"left\"/><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning)\n",
    "____________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning\n",
    "\n",
    "\n",
    "Apart from supervised and unsupervised ML there is reinforcement learning which is an explorative machine learning algorithm where an agent learns strategies to maximize its rewards by interacting with an environment which either penalizes or rewards every action of the agent. Typically, the aim is to exploit by exporing new states which requires strategies for trade-offs between exploitation and exploration for which different algorithms exist (Q-learning, SARSA). RL can be applied in scenarios where a lot of data is available, e.g. in gameplay and robotics where simulations are readily available. \n",
    "\n",
    "Some key concepts describing a RL problem: \n",
    "\n",
    "- Agent: wants to maximize its reward through exploration of the environment \n",
    "- Environment: Physical world in which the agent operates\n",
    "- State: Current situation of the agent\n",
    "- Reward: Feedback from the environment\n",
    "- Policy: Method to map agentâ€™s state to actions\n",
    "- Value: Future reward that an agent would receive by taking an action in a particular state\n",
    "\n",
    "\n",
    "<img src=\"reinforcement_learning.jpg\" style=\"width: 300px;\" align=\"left\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning <a class=\"anchor\" id=\"nine\"></a>\n",
    "### Multilayer perceptron\n",
    "\n",
    "Deep Learning is a powerful division of Machine Learning where artifical neural networks (ANN) are at the heart of the learning algorithms inspired by the way neurons works in our brain. An ANN consists of several layers where each layer is made up of nodes with weighted connections to other nodes from different layers. Simply put, an ANN can be understood as a function with parameters which maps an input vector to an output vector or scalar. \n",
    "\n",
    "Here are the main components of the ANN: \n",
    "\n",
    "- __input layer__: fed with the different dimensions/features of the training data, which is then passed on and transformed through the network through to the \n",
    "- __output layer__:  contains the final result from the function. \n",
    "- __hidden layer__: layers between input and output layer\n",
    "- __node__: each layer consists of nodes which are connected to nodes from other layers through connections and weights. At all nodes from the hidden layer and the output layer a dot product computation is performed between the output values ($x_i$) of the previous nodes and weights ($w_i$) attached to every connection. Then, an activation function is applied which maps the dot product either to 0 or 1 or depending on the scenario, to a probability value in the case of nodes from the ouput layer\n",
    "- __weight__: weights: parameters of the neural network which are learned during training. Each node comes with an initialized set of weights which is used for the dot product computation. The weight is the strength at which the input signal is passed on\n",
    "- __loss function__: as with every other ML algorithm, the goal is to learn the model parameters (*weights*) through multiple training iterations in which the error between the current outcome and the expected outcome is reduced\n",
    "- __activation functions__: In every node, after the dot product computation, an activation function is applied which maps the output to a value between 0 and 1 for example. This is determined by the fact if the signal fires or does not, so if it is significant or not or sth. in the middle\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"mlp.png\" style=\"width: 300px;\" align=\"left\"/>\n",
    "\n",
    "\n",
    "#### Backpropogation\n",
    "\n",
    "\n",
    "The objective in trianing a neural network is is to reduce the error between the expected outcome and model outcome with the current parameters. The first iteration, typically starts with random parameter initializations which lead to some model outcome and the resulting error. The partial derivates of the error function with regard to each weight parameter are then used to adjust the weights such that the error is iteratively minimized, and a cat is recognized as a cat, \n",
    "Bropagation refers to the propagtion of errors through the layers and the utilization of the partial derivative of the gradient from a previous layer in the current layer. \n",
    "\n",
    "The steps involved in backpropagation can be boiled down to 3 steps which happen in a single iteration until no improvement occurs and convergence has been achieved: \n",
    "1. Computing the error with the current weight parameters\n",
    "2. Computing the partial derivatives (gradients) of the error function with respect to each of the weight parameters. \n",
    "3. Tweaking the weights such that error is reduced.\n",
    "\n",
    "Illustratively, the objective is to find the local or global minimum of the error function. The gradient describes the steepness of the error function, so during the training process we want to go downhill from the initial random state of the weight parameters. \n",
    "<br><br>\n",
    "\n",
    "\n",
    " <img src=\"gradient_descent.jpeg\" style=\"width: 300px;\" align=\"left\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer vision basics\n",
    "\n",
    "In computer vision, the Convoluted Neural Network (CNN) is commonly used for image recognition. \n",
    "\n",
    "A CNN takes as input an image of n x m pixels with 3 color channels RGB, resulting in a n x m x 3 image-vector. The output vector encodes the information whether it is a dog, a cat, a bird or whatever image objects the model is trained on. \n",
    "A CNN consists of 3 types of hidden layers: convolution, pooling and fully connected feed forward layer. \n",
    "\n",
    "<img src=\"cnn_filter.png\" style=\"width: 400px;\" align=\"left\"/>\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "In the convolution layer, a kernel (filter) is used to scan the image and extract features of the image by applying dot product operations to extract features. Typically, the kernel size is 3x3 with a stride of 1 or 2. This means that the kernel will slide through the image with a stride of 1. In the lower convolution layers, kernels will extract low-level features like lines, edges, curves, in the upper levels features will be more sophisticated like the unique shapes, color and other characteristics of eyes, nose, lips etc. belonging to a specific person. The result from the convoultions applied are reflected in the feature maps and the number of the kernels will be equal to the number of feature maps that a convolution layer generates. \n",
    "\n",
    "Examples of convolutions to extract features:\n",
    "\n",
    "\n",
    "<img src=\"cnn_before_after.png\" style=\"width: 400px;\" align=\"left\"/>\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "A convolution layer is followed by a pooling layer which further reduces the amount of information and performs noise-cancellations by applying max or average pooling. Finally, a fully connected feed forward layer is applied where the image n x m x 3 dimensional image-vector is flattened and then mapped to the output vector containing the information on the image class. \n",
    "<br><br>\n",
    "\n",
    "<img src=\"cnn_pooling.png\" style=\"width: 400px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNN)\n",
    "\n",
    "RNNs are capable of processing sequence data. Sequence data is a series of input data where the order plays a role, e.g. in text pieces and time series data. So at every given point the output of the RNN as the next sequence item is dependent on not just the input vector but also a state which contains the context information based on all previous inputs and the resulting states. So the RNN has a got memory. The following image shows a representation of the RNN which can be unrolled in time: \n",
    "\n",
    "<img src=\"rnn.png\" style=\"width: 450px;\" align=\"left\"/><br><br><br><br><br><br><br><br>\n",
    "\n",
    "What it means is that at time t=0, the RNN takes X(0) and returns the hidden state h(0) which together with X(1) returns h(1). X(2) uses h(1) to generate h(2) and so on. This way the RNN remembers the states which influence the output of the model. \n",
    "\n",
    "\n",
    "This simple memory cell was not able to retain the state over longer sequences. The breakthrough to RNNs was brough about by the introduction of LSTM and later GRU sells which are a simplification of the former but perform as good as LSTMs. LSTM stands for Long Short Term Memory and has got an input gate, a forget gate and an output gate. The input gate controls what information should enter the long term memory, a forget gate controls what can be forgotten, and the output gate determines which information should be retrieved from the memory. The GRU cell works in a similar way but has no output gate, so retrieves all of the context but which will lead to the same predictions since GRU performs just as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)\n",
    "\n",
    "NLP is a field of AI and ML that enables machines to read, understand and derives meaning from human languages. Data generated from articles, conversations and social media are examples of unstructured text data which is messy and hard to manipulate. Thanks to advances in NLP and ML, there are various techniques that enable machines to go beyond the analysis of key words and understand context and semantic nuances behind a text, e.g.figures of speech like irony.\n",
    "Examples of Use cases for NLP are: \n",
    "- machine translation\n",
    "- sentiment analysis e.g. in social media or based on product reviews\n",
    "- cognitiive Assistants and Chatbots \n",
    "- Spam classification\n",
    "- identitifying fake news\n",
    "- diagnosis in health care through improved documentation of health records\n",
    "\n",
    "\n",
    "#### Bag of words\n",
    "Bag words represent occurrence matrices for a sentence or document, disregarding grammar or word order. They are used as features for training a classfier.\n",
    "\n",
    "#### Tokenization\n",
    "To to be able to process unstructured data, text is broken down into pieces of words and terms called tokens, often split by space. In this process certain characters, like punctuation is removed.The process of segmenting text into sentences and words is called tokenization. Each token then obtains a clear index which plays a role when retrieving the document which matches a sentence searched. Tokens are also the building blocks of input sequences fed into neural networks and Machine learning algorithms. \n",
    "\n",
    "#### Stemming and Lemmatization\n",
    "Stemming and Lemmatization aim at unifying words which have the same semantic root. This helps to reduce noise from grammar, spelling errors, plural forms. Stemming cuts the word bearing the risk that the meaning gets changed while Lemmatization neutralizes the grammatical tense, e.g. <br>\n",
    "__Stemming__: Cars -> Car <br>\n",
    "__Lemmatization__: Caring -> Care\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"token_nlp.jpg\" style=\"width: 400px;\" align=\"left\"/>\n",
    "<br><br><br><br><br><br><br>\n",
    "Vocabulary: <br>\n",
    "<img src=\"vocab_nlp.jpeg\" style=\"width: 400px;\" align=\"left\"/>\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "#### Word embbeding: <br>\n",
    "\n",
    "Words are represented as numerical vectors which are called embeddings in a vectorspace. With learned vectors you can apply algebra which help infer new relations between the words, e.g. a man is to king, the way a woman is to queen. Or the relations between capital and country where geographically nearer countries will be also closer located in the vectorspace. \n",
    "\n",
    "\n",
    "<img src=\"word_embbedings.jpeg\" style=\"width: 400px;\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning <a class=\"anchor\" id=\"ten\"></a>\n",
    "[Kaggle](https://www.kaggle.com/): Datasets, Courses and Datascience challenges<br>\n",
    "[Towardsdatascience](https://towardsdatascience.com/): Blogs about Datascience"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
